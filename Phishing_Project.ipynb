{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrussell93/prework-about-me/blob/master/Phishing_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Phishing Project\n",
        "#Carter Logistic Regression code\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, classification_report\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/MyDrive/PhishingCSV.csv\"\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "# Split the dataset into input (X) and the target output (y)\n",
        "X = data.drop(['Result', 'id'], axis=1)  # X contains all columns except the Results and id\n",
        "y = data['Result']  # y contains only the Results\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Linear Regression\n",
        "# Create a Linear Regression model\n",
        "linear_reg = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "linear_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_linear = linear_reg.predict(X_test)\n",
        "\n",
        "# Evaluate Linear Regression model\n",
        "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
        "print(f\"Linear Regression Mean Squared Error: {mse_linear:.2f}\")\n",
        "\n",
        "# Logistic Regression\n",
        "# Create a Logistic Regression model\n",
        "logistic_reg = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "logistic_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_logistic = logistic_reg.predict(X_test)\n",
        "\n",
        "# Evaluate Logistic Regression model\n",
        "logistic_accuracy = accuracy_score(y_test, y_pred_logistic)\n",
        "print(f\"Logistic Regression Accuracy: {logistic_accuracy:.2f}\")\n",
        "\n",
        "# Display Logistic Regression classification report\n",
        "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, y_pred_logistic))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8HfxAA2s_Ee",
        "outputId": "31853cda-f0c3-48e7-9844-c4e4bf205594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Linear Regression Mean Squared Error: 0.31\n",
            "Logistic Regression Accuracy: 0.92\n",
            "Logistic Regression Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.92      0.89      0.91      1014\n",
            "           1       0.91      0.94      0.92      1197\n",
            "\n",
            "    accuracy                           0.92      2211\n",
            "   macro avg       0.92      0.91      0.92      2211\n",
            "weighted avg       0.92      0.92      0.92      2211\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Carter Decision Tree Classifier Code\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path =\"/content/drive/MyDrive/PhishingCSV.csv\"\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "\n",
        "# Split the dataset into input (X) and the target output (y)\n",
        "X = data.drop(['Result', 'id'], axis=1)  # X contains all columns except the Results and id\n",
        "y = data['Result']  # y contains only the Results\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display classification report\n",
        "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y25EY-AOs-5z",
        "outputId": "4bfe72f0-8eae-446e-821b-3a90f9746537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Accuracy: 0.96\n",
            "Decision Tree Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.97      0.95      0.96      1014\n",
            "           1       0.96      0.98      0.97      1197\n",
            "\n",
            "    accuracy                           0.96      2211\n",
            "   macro avg       0.96      0.96      0.96      2211\n",
            "weighted avg       0.96      0.96      0.96      2211\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nick's Attempt at Single Layer Neural Network with PyTorch\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from google.colab import drive\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import imageio\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/Project Proposal/csv_result-phpV5QYya.csv'\n",
        "phish = pd.read_csv(file_path)\n",
        "# Standardize the data\n",
        "#standard = StandardScaler()\n",
        "# Create a list of feature\n",
        "variable_list = ['having_IP_Address','URL_Length','Shortining_Service','having_At_Symbol','double_slash_redirecting','Prefix_Suffix','having_Sub_Domain','SSLfinal_State','Domain_registeration_length','Favicon','port','HTTPS_token','Request_URL','URL_of_Anchor','Links_in_tags','SFH','Submitting_to_email','Abnormal_URL','Redirect','on_mouseover','RightClick','popUpWidnow','Iframe','age_of_domain','DNSRecord','web_traffic','Page_Rank','Google_Index','Links_pointing_to_page','Statistical_report']\n",
        "\n",
        "# Extract features and labels\n",
        "x = phish.drop(['Result', 'id'], axis = 1)\n",
        "\n",
        "y = phish['Result']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "# Turn data set into seperate training and validation sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# Reshape the data to aviod problems with tensor creation\n",
        "#y_train = y_train.values.reshape(-1,1)\n",
        "#y_test = y_test.values.reshape(-1,1)\n",
        "# Stack the data\n",
        "x_train_std_np = np.c_[np.ones((len(y_train), 1)), x_train[variable_list]]\n",
        "#x_train_std_np = np.c_[np.ones((len(y_train), 1)), x_train[variable_list]]\n",
        "x_test_std_np = np.c_[np.ones((len(y_test), 1)), x_test[variable_list]]\n",
        "# Tensor time\n",
        "#x_train_tensor = torch.tensor(x_train_std_np, dtype = torch.float)\n",
        "#x_test_tensor = torch.tensor(x_test_std_np, dtype = torch.float)\n",
        "#y_train_tensor = torch.tensor(y_train, dtype = torch.float)\n",
        "#y_test_tensor = torch.tensor(y_test, dtype = torch.float)\n",
        "# Tensor time\n",
        "x_train_tensor = torch.tensor(x_train_std_np, dtype = torch.float)\n",
        "x_test_tensor = torch.tensor(x_test_std_np, dtype = torch.float)\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "y_test_tensor = torch.tensor(y_test)\n",
        "# Perform model creation\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(31, 1),  # 31 input features, 1 output neuron for binary classification\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "#[parameter.shape for parameter in model.parameters()]\n",
        "# Create function to be called when performing training loop process\n",
        "def training(epochs,optimizer,model,loss,x_train,x_test,y_train,y_test):\n",
        "  for epoch in range(1,epochs+1):\n",
        "    model_train = model(x_train)\n",
        "    train_loss = loss_fn(model_train.squeeze(), y_train.float())\n",
        "    #train_loss = loss(model_train,y_train)\n",
        "    model_test = model(x_test)\n",
        "    test_loss = loss_fn(model_test.squeeze(), y_test.float())\n",
        "    #test_loss = loss(model_test,y_test)\n",
        "    # Perform optimization\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    # Dispay train and test losses at intervals of 1000\n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "      print((\"Iteration: \", epoch, \"Training Loss: \", train_loss.item(), \"Testing Loss: \", test_loss.item()))\n",
        "# Perform training on one layer with 32\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "loss_fn = nn.BCELoss()\n",
        "#loss_fn = nn.CrossEntropyLoss()\n",
        "training(epochs = 1500,\n",
        "         optimizer = optimizer,\n",
        "         model = model,\n",
        "         loss = loss_fn,\n",
        "         x_train = x_train_tensor,\n",
        "         x_test = x_test_tensor,\n",
        "         y_train = y_train_tensor,\n",
        "         y_test = y_test_tensor\n",
        ")\n",
        "\n",
        "# Apply the trained model to the test set for evaluation\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    y_pred = (model(x_test_tensor).squeeze() > 0.5).float()\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test_tensor, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test_tensor, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test_tensor, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_tensor, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKySZEFoOs8S",
        "outputId": "0bd8a7aa-da0c-4ff5-e8e0-897e3535e2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "('Iteration: ', 1, 'Training Loss: ', 0.7454620003700256, 'Testing Loss: ', 0.7539750337600708)\n",
            "('Iteration: ', 10, 'Training Loss: ', 0.7159147262573242, 'Testing Loss: ', 0.724607527256012)\n",
            "('Iteration: ', 20, 'Training Loss: ', 0.6857696771621704, 'Testing Loss: ', 0.6945773363113403)\n",
            "('Iteration: ', 30, 'Training Loss: ', 0.6581898331642151, 'Testing Loss: ', 0.6670538783073425)\n",
            "('Iteration: ', 40, 'Training Loss: ', 0.6329386830329895, 'Testing Loss: ', 0.6418231129646301)\n",
            "('Iteration: ', 50, 'Training Loss: ', 0.6098031401634216, 'Testing Loss: ', 0.6186871528625488)\n",
            "('Iteration: ', 60, 'Training Loss: ', 0.5885880589485168, 'Testing Loss: ', 0.5974608063697815)\n",
            "('Iteration: ', 70, 'Training Loss: ', 0.5691139101982117, 'Testing Loss: ', 0.5779711008071899)\n",
            "('Iteration: ', 80, 'Training Loss: ', 0.5512160658836365, 'Testing Loss: ', 0.5600573420524597)\n",
            "('Iteration: ', 90, 'Training Loss: ', 0.5347437262535095, 'Testing Loss: ', 0.5435711145401001)\n",
            "('Iteration: ', 100, 'Training Loss: ', 0.5195598602294922, 'Testing Loss: ', 0.5283765196800232)\n",
            "('Iteration: ', 110, 'Training Loss: ', 0.5055402517318726, 'Testing Loss: ', 0.5143499970436096)\n",
            "('Iteration: ', 120, 'Training Loss: ', 0.492573082447052, 'Testing Loss: ', 0.5013797283172607)\n",
            "('Iteration: ', 130, 'Training Loss: ', 0.48055753111839294, 'Testing Loss: ', 0.48936471343040466)\n",
            "('Iteration: ', 140, 'Training Loss: ', 0.46940329670906067, 'Testing Loss: ', 0.47821444272994995)\n",
            "('Iteration: ', 150, 'Training Loss: ', 0.4590294659137726, 'Testing Loss: ', 0.4678475260734558)\n",
            "('Iteration: ', 160, 'Training Loss: ', 0.4493635594844818, 'Testing Loss: ', 0.4581913352012634)\n",
            "('Iteration: ', 170, 'Training Loss: ', 0.44034096598625183, 'Testing Loss: ', 0.44918060302734375)\n",
            "('Iteration: ', 180, 'Training Loss: ', 0.4319036900997162, 'Testing Loss: ', 0.4407571256160736)\n",
            "('Iteration: ', 190, 'Training Loss: ', 0.42399996519088745, 'Testing Loss: ', 0.43286871910095215)\n",
            "('Iteration: ', 200, 'Training Loss: ', 0.4165832996368408, 'Testing Loss: ', 0.425468772649765)\n",
            "('Iteration: ', 210, 'Training Loss: ', 0.40961214900016785, 'Testing Loss: ', 0.41851526498794556)\n",
            "('Iteration: ', 220, 'Training Loss: ', 0.40304920077323914, 'Testing Loss: ', 0.411970853805542)\n",
            "('Iteration: ', 230, 'Training Loss: ', 0.39686086773872375, 'Testing Loss: ', 0.4058016836643219)\n",
            "('Iteration: ', 240, 'Training Loss: ', 0.3910169303417206, 'Testing Loss: ', 0.3999774158000946)\n",
            "('Iteration: ', 250, 'Training Loss: ', 0.3854902982711792, 'Testing Loss: ', 0.39447078108787537)\n",
            "('Iteration: ', 260, 'Training Loss: ', 0.3802563548088074, 'Testing Loss: ', 0.38925701379776)\n",
            "('Iteration: ', 270, 'Training Loss: ', 0.37529292702674866, 'Testing Loss: ', 0.3843139111995697)\n",
            "('Iteration: ', 280, 'Training Loss: ', 0.37057992815971375, 'Testing Loss: ', 0.3796212673187256)\n",
            "('Iteration: ', 290, 'Training Loss: ', 0.36609911918640137, 'Testing Loss: ', 0.37516075372695923)\n",
            "('Iteration: ', 300, 'Training Loss: ', 0.3618339002132416, 'Testing Loss: ', 0.3709157407283783)\n",
            "('Iteration: ', 310, 'Training Loss: ', 0.357769250869751, 'Testing Loss: ', 0.3668711483478546)\n",
            "('Iteration: ', 320, 'Training Loss: ', 0.3538914620876312, 'Testing Loss: ', 0.36301326751708984)\n",
            "('Iteration: ', 330, 'Training Loss: ', 0.35018786787986755, 'Testing Loss: ', 0.3593294024467468)\n",
            "('Iteration: ', 340, 'Training Loss: ', 0.3466472029685974, 'Testing Loss: ', 0.3558081388473511)\n",
            "('Iteration: ', 350, 'Training Loss: ', 0.34325873851776123, 'Testing Loss: ', 0.35243889689445496)\n",
            "('Iteration: ', 360, 'Training Loss: ', 0.34001293778419495, 'Testing Loss: ', 0.34921202063560486)\n",
            "('Iteration: ', 370, 'Training Loss: ', 0.3369009792804718, 'Testing Loss: ', 0.346118688583374)\n",
            "('Iteration: ', 380, 'Training Loss: ', 0.333914577960968, 'Testing Loss: ', 0.34315064549446106)\n",
            "('Iteration: ', 390, 'Training Loss: ', 0.33104637265205383, 'Testing Loss: ', 0.34030044078826904)\n",
            "('Iteration: ', 400, 'Training Loss: ', 0.32828935980796814, 'Testing Loss: ', 0.3375612199306488)\n",
            "('Iteration: ', 410, 'Training Loss: ', 0.32563719153404236, 'Testing Loss: ', 0.3349264860153198)\n",
            "('Iteration: ', 420, 'Training Loss: ', 0.32308387756347656, 'Testing Loss: ', 0.33239027857780457)\n",
            "('Iteration: ', 430, 'Training Loss: ', 0.32062387466430664, 'Testing Loss: ', 0.32994717359542847)\n",
            "('Iteration: ', 440, 'Training Loss: ', 0.31825223565101624, 'Testing Loss: ', 0.3275921046733856)\n",
            "('Iteration: ', 450, 'Training Loss: ', 0.31596413254737854, 'Testing Loss: ', 0.32532021403312683)\n",
            "('Iteration: ', 460, 'Training Loss: ', 0.31375518441200256, 'Testing Loss: ', 0.32312721014022827)\n",
            "('Iteration: ', 470, 'Training Loss: ', 0.3116212785243988, 'Testing Loss: ', 0.32100898027420044)\n",
            "('Iteration: ', 480, 'Training Loss: ', 0.30955860018730164, 'Testing Loss: ', 0.3189616799354553)\n",
            "('Iteration: ', 490, 'Training Loss: ', 0.30756357312202454, 'Testing Loss: ', 0.3169817626476288)\n",
            "('Iteration: ', 500, 'Training Loss: ', 0.3056328296661377, 'Testing Loss: ', 0.3150659203529358)\n",
            "('Iteration: ', 510, 'Training Loss: ', 0.30376335978507996, 'Testing Loss: ', 0.31321096420288086)\n",
            "('Iteration: ', 520, 'Training Loss: ', 0.3019521236419678, 'Testing Loss: ', 0.31141409277915955)\n",
            "('Iteration: ', 530, 'Training Loss: ', 0.30019646883010864, 'Testing Loss: ', 0.30967244505882263)\n",
            "('Iteration: ', 540, 'Training Loss: ', 0.29849377274513245, 'Testing Loss: ', 0.3079835772514343)\n",
            "('Iteration: ', 550, 'Training Loss: ', 0.2968415915966034, 'Testing Loss: ', 0.30634498596191406)\n",
            "('Iteration: ', 560, 'Training Loss: ', 0.2952377498149872, 'Testing Loss: ', 0.3047545254230499)\n",
            "('Iteration: ', 570, 'Training Loss: ', 0.2936800718307495, 'Testing Loss: ', 0.30320996046066284)\n",
            "('Iteration: ', 580, 'Training Loss: ', 0.29216650128364563, 'Testing Loss: ', 0.3017093241214752)\n",
            "('Iteration: ', 590, 'Training Loss: ', 0.2906952500343323, 'Testing Loss: ', 0.30025067925453186)\n",
            "('Iteration: ', 600, 'Training Loss: ', 0.28926441073417664, 'Testing Loss: ', 0.29883235692977905)\n",
            "('Iteration: ', 610, 'Training Loss: ', 0.287872314453125, 'Testing Loss: ', 0.2974525988101959)\n",
            "('Iteration: ', 620, 'Training Loss: ', 0.286517471075058, 'Testing Loss: ', 0.2961098253726959)\n",
            "('Iteration: ', 630, 'Training Loss: ', 0.28519827127456665, 'Testing Loss: ', 0.2948025166988373)\n",
            "('Iteration: ', 640, 'Training Loss: ', 0.2839133143424988, 'Testing Loss: ', 0.29352930188179016)\n",
            "('Iteration: ', 650, 'Training Loss: ', 0.28266122937202454, 'Testing Loss: ', 0.29228875041007996)\n",
            "('Iteration: ', 660, 'Training Loss: ', 0.281440794467926, 'Testing Loss: ', 0.29107967019081116)\n",
            "('Iteration: ', 670, 'Training Loss: ', 0.28025078773498535, 'Testing Loss: ', 0.2899007797241211)\n",
            "('Iteration: ', 680, 'Training Loss: ', 0.27908995747566223, 'Testing Loss: ', 0.2887510061264038)\n",
            "('Iteration: ', 690, 'Training Loss: ', 0.2779572606086731, 'Testing Loss: ', 0.28762924671173096)\n",
            "('Iteration: ', 700, 'Training Loss: ', 0.27685171365737915, 'Testing Loss: ', 0.2865343987941742)\n",
            "('Iteration: ', 710, 'Training Loss: ', 0.2757723033428192, 'Testing Loss: ', 0.2854654788970947)\n",
            "('Iteration: ', 720, 'Training Loss: ', 0.2747180163860321, 'Testing Loss: ', 0.2844216525554657)\n",
            "('Iteration: ', 730, 'Training Loss: ', 0.27368804812431335, 'Testing Loss: ', 0.2834019660949707)\n",
            "('Iteration: ', 740, 'Training Loss: ', 0.27268144488334656, 'Testing Loss: ', 0.2824055254459381)\n",
            "('Iteration: ', 750, 'Training Loss: ', 0.271697461605072, 'Testing Loss: ', 0.28143152594566345)\n",
            "('Iteration: ', 760, 'Training Loss: ', 0.27073532342910767, 'Testing Loss: ', 0.2804793119430542)\n",
            "('Iteration: ', 770, 'Training Loss: ', 0.269794225692749, 'Testing Loss: ', 0.27954795956611633)\n",
            "('Iteration: ', 780, 'Training Loss: ', 0.26887357234954834, 'Testing Loss: ', 0.27863696217536926)\n",
            "('Iteration: ', 790, 'Training Loss: ', 0.26797258853912354, 'Testing Loss: ', 0.27774545550346375)\n",
            "('Iteration: ', 800, 'Training Loss: ', 0.26709064841270447, 'Testing Loss: ', 0.2768729329109192)\n",
            "('Iteration: ', 810, 'Training Loss: ', 0.2662271559238434, 'Testing Loss: ', 0.27601873874664307)\n",
            "('Iteration: ', 820, 'Training Loss: ', 0.26538145542144775, 'Testing Loss: ', 0.27518224716186523)\n",
            "('Iteration: ', 830, 'Training Loss: ', 0.26455312967300415, 'Testing Loss: ', 0.2743629813194275)\n",
            "('Iteration: ', 840, 'Training Loss: ', 0.26374146342277527, 'Testing Loss: ', 0.2735603451728821)\n",
            "('Iteration: ', 850, 'Training Loss: ', 0.2629460394382477, 'Testing Loss: ', 0.272773802280426)\n",
            "('Iteration: ', 860, 'Training Loss: ', 0.2621663510799408, 'Testing Loss: ', 0.27200284600257874)\n",
            "('Iteration: ', 870, 'Training Loss: ', 0.26140180230140686, 'Testing Loss: ', 0.2712470293045044)\n",
            "('Iteration: ', 880, 'Training Loss: ', 0.260652095079422, 'Testing Loss: ', 0.2705058753490448)\n",
            "('Iteration: ', 890, 'Training Loss: ', 0.2599167227745056, 'Testing Loss: ', 0.2697790563106537)\n",
            "('Iteration: ', 900, 'Training Loss: ', 0.2591952085494995, 'Testing Loss: ', 0.2690660059452057)\n",
            "('Iteration: ', 910, 'Training Loss: ', 0.2584872841835022, 'Testing Loss: ', 0.26836642622947693)\n",
            "('Iteration: ', 920, 'Training Loss: ', 0.2577924430370331, 'Testing Loss: ', 0.26767978072166443)\n",
            "('Iteration: ', 930, 'Training Loss: ', 0.2571103274822235, 'Testing Loss: ', 0.2670058608055115)\n",
            "('Iteration: ', 940, 'Training Loss: ', 0.2564406096935272, 'Testing Loss: ', 0.266344279050827)\n",
            "('Iteration: ', 950, 'Training Loss: ', 0.2557828724384308, 'Testing Loss: ', 0.26569461822509766)\n",
            "('Iteration: ', 960, 'Training Loss: ', 0.2551368772983551, 'Testing Loss: ', 0.2650565505027771)\n",
            "('Iteration: ', 970, 'Training Loss: ', 0.25450220704078674, 'Testing Loss: ', 0.26442980766296387)\n",
            "('Iteration: ', 980, 'Training Loss: ', 0.253878653049469, 'Testing Loss: ', 0.2638140320777893)\n",
            "('Iteration: ', 990, 'Training Loss: ', 0.2532658576965332, 'Testing Loss: ', 0.2632090151309967)\n",
            "('Iteration: ', 1000, 'Training Loss: ', 0.2526635229587555, 'Testing Loss: ', 0.262614369392395)\n",
            "('Iteration: ', 1010, 'Training Loss: ', 0.2520713806152344, 'Testing Loss: ', 0.26202985644340515)\n",
            "('Iteration: ', 1020, 'Training Loss: ', 0.25148919224739075, 'Testing Loss: ', 0.261455237865448)\n",
            "('Iteration: ', 1030, 'Training Loss: ', 0.25091660022735596, 'Testing Loss: ', 0.2608902156352997)\n",
            "('Iteration: ', 1040, 'Training Loss: ', 0.25035351514816284, 'Testing Loss: ', 0.2603345513343811)\n",
            "('Iteration: ', 1050, 'Training Loss: ', 0.24979956448078156, 'Testing Loss: ', 0.25978797674179077)\n",
            "('Iteration: ', 1060, 'Training Loss: ', 0.24925456941127777, 'Testing Loss: ', 0.25925034284591675)\n",
            "('Iteration: ', 1070, 'Training Loss: ', 0.24871826171875, 'Testing Loss: ', 0.25872135162353516)\n",
            "('Iteration: ', 1080, 'Training Loss: ', 0.2481905072927475, 'Testing Loss: ', 0.2582007944583893)\n",
            "('Iteration: ', 1090, 'Training Loss: ', 0.2476710081100464, 'Testing Loss: ', 0.2576884925365448)\n",
            "('Iteration: ', 1100, 'Training Loss: ', 0.24715964496135712, 'Testing Loss: ', 0.2571842670440674)\n",
            "('Iteration: ', 1110, 'Training Loss: ', 0.2466561198234558, 'Testing Loss: ', 0.25668784976005554)\n",
            "('Iteration: ', 1120, 'Training Loss: ', 0.2461603283882141, 'Testing Loss: ', 0.25619909167289734)\n",
            "('Iteration: ', 1130, 'Training Loss: ', 0.2456720471382141, 'Testing Loss: ', 0.25571781396865845)\n",
            "('Iteration: ', 1140, 'Training Loss: ', 0.24519114196300507, 'Testing Loss: ', 0.25524383783340454)\n",
            "('Iteration: ', 1150, 'Training Loss: ', 0.24471737444400787, 'Testing Loss: ', 0.2547769546508789)\n",
            "('Iteration: ', 1160, 'Training Loss: ', 0.2442506104707718, 'Testing Loss: ', 0.254317045211792)\n",
            "('Iteration: ', 1170, 'Training Loss: ', 0.2437906712293625, 'Testing Loss: ', 0.25386396050453186)\n",
            "('Iteration: ', 1180, 'Training Loss: ', 0.24333742260932922, 'Testing Loss: ', 0.2534174621105194)\n",
            "('Iteration: ', 1190, 'Training Loss: ', 0.24289068579673767, 'Testing Loss: ', 0.25297749042510986)\n",
            "('Iteration: ', 1200, 'Training Loss: ', 0.24245035648345947, 'Testing Loss: ', 0.2525438666343689)\n",
            "('Iteration: ', 1210, 'Training Loss: ', 0.2420162707567215, 'Testing Loss: ', 0.25211644172668457)\n",
            "('Iteration: ', 1220, 'Training Loss: ', 0.2415882796049118, 'Testing Loss: ', 0.25169506669044495)\n",
            "('Iteration: ', 1230, 'Training Loss: ', 0.24116621911525726, 'Testing Loss: ', 0.25127965211868286)\n",
            "('Iteration: ', 1240, 'Training Loss: ', 0.2407500445842743, 'Testing Loss: ', 0.250870019197464)\n",
            "('Iteration: ', 1250, 'Training Loss: ', 0.24033954739570618, 'Testing Loss: ', 0.2504660487174988)\n",
            "('Iteration: ', 1260, 'Training Loss: ', 0.23993463814258575, 'Testing Loss: ', 0.2500676214694977)\n",
            "('Iteration: ', 1270, 'Training Loss: ', 0.23953521251678467, 'Testing Loss: ', 0.24967467784881592)\n",
            "('Iteration: ', 1280, 'Training Loss: ', 0.23914115130901337, 'Testing Loss: ', 0.24928703904151917)\n",
            "('Iteration: ', 1290, 'Training Loss: ', 0.23875227570533752, 'Testing Loss: ', 0.24890457093715668)\n",
            "('Iteration: ', 1300, 'Training Loss: ', 0.23836858570575714, 'Testing Loss: ', 0.24852722883224487)\n",
            "('Iteration: ', 1310, 'Training Loss: ', 0.23798984289169312, 'Testing Loss: ', 0.24815483391284943)\n",
            "('Iteration: ', 1320, 'Training Loss: ', 0.23761607706546783, 'Testing Loss: ', 0.24778735637664795)\n",
            "('Iteration: ', 1330, 'Training Loss: ', 0.23724707961082458, 'Testing Loss: ', 0.2474246472120285)\n",
            "('Iteration: ', 1340, 'Training Loss: ', 0.23688285052776337, 'Testing Loss: ', 0.2470666915178299)\n",
            "('Iteration: ', 1350, 'Training Loss: ', 0.23652324080467224, 'Testing Loss: ', 0.246713325381279)\n",
            "('Iteration: ', 1360, 'Training Loss: ', 0.23616814613342285, 'Testing Loss: ', 0.24636441469192505)\n",
            "('Iteration: ', 1370, 'Training Loss: ', 0.23581750690937042, 'Testing Loss: ', 0.24601995944976807)\n",
            "('Iteration: ', 1380, 'Training Loss: ', 0.2354712337255478, 'Testing Loss: ', 0.2456798106431961)\n",
            "('Iteration: ', 1390, 'Training Loss: ', 0.23512917757034302, 'Testing Loss: ', 0.2453439086675644)\n",
            "('Iteration: ', 1400, 'Training Loss: ', 0.2347913384437561, 'Testing Loss: ', 0.24501214921474457)\n",
            "('Iteration: ', 1410, 'Training Loss: ', 0.2344575971364975, 'Testing Loss: ', 0.24468450248241425)\n",
            "('Iteration: ', 1420, 'Training Loss: ', 0.23412790894508362, 'Testing Loss: ', 0.24436086416244507)\n",
            "('Iteration: ', 1430, 'Training Loss: ', 0.23380213975906372, 'Testing Loss: ', 0.24404112994670868)\n",
            "('Iteration: ', 1440, 'Training Loss: ', 0.23348025977611542, 'Testing Loss: ', 0.2437252700328827)\n",
            "('Iteration: ', 1450, 'Training Loss: ', 0.23316216468811035, 'Testing Loss: ', 0.24341316521167755)\n",
            "('Iteration: ', 1460, 'Training Loss: ', 0.23284782469272614, 'Testing Loss: ', 0.24310480058193207)\n",
            "('Iteration: ', 1470, 'Training Loss: ', 0.2325371503829956, 'Testing Loss: ', 0.2428000569343567)\n",
            "('Iteration: ', 1480, 'Training Loss: ', 0.2322300523519516, 'Testing Loss: ', 0.24249888956546783)\n",
            "('Iteration: ', 1490, 'Training Loss: ', 0.23192647099494934, 'Testing Loss: ', 0.24220125377178192)\n",
            "('Iteration: ', 1500, 'Training Loss: ', 0.23162634670734406, 'Testing Loss: ', 0.2419070303440094)\n",
            "Accuracy: 0.911352329262777\n",
            "Precision: 0.9013632718524459\n",
            "Recall: 0.9390142021720969\n",
            "Confusion Matrix:\n",
            " [[ 891  123]\n",
            " [  73 1124]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nick's Attempt at Double layer Neural Network with PyTorch\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from google.colab import drive\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import imageio\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/drive/MyDrive/PhishingCSV.csv\"\n",
        "phish = pd.read_csv(file_path)\n",
        "# Standardize the data\n",
        "#standard = StandardScaler()\n",
        "# Create a list of feature\n",
        "variable_list = ['having_IP_Address','URL_Length','Shortining_Service','having_At_Symbol','double_slash_redirecting','Prefix_Suffix','having_Sub_Domain','SSLfinal_State','Domain_registeration_length','Favicon','port','HTTPS_token','Request_URL','URL_of_Anchor','Links_in_tags','SFH','Submitting_to_email','Abnormal_URL','Redirect','on_mouseover','RightClick','popUpWidnow','Iframe','age_of_domain','DNSRecord','web_traffic','Page_Rank','Google_Index','Links_pointing_to_page','Statistical_report']\n",
        "\n",
        "# Extract features and labels\n",
        "x = phish.drop(['Result', 'id'], axis = 1)\n",
        "\n",
        "y = phish['Result']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "# Turn data set into seperate training and validation sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# Reshape the data to aviod problems with tensor creation\n",
        "#y_train = y_train.values.reshape(-1,1)\n",
        "#y_test = y_test.values.reshape(-1,1)\n",
        "# Stack the data\n",
        "x_train_std_np = np.c_[np.ones((len(y_train), 1)), x_train[variable_list]]\n",
        "#x_train_std_np = np.c_[np.ones((len(y_train), 1)), x_train[variable_list]]\n",
        "x_test_std_np = np.c_[np.ones((len(y_test), 1)), x_test[variable_list]]\n",
        "# Tensor time\n",
        "#x_train_tensor = torch.tensor(x_train_std_np, dtype = torch.float)\n",
        "#x_test_tensor = torch.tensor(x_test_std_np, dtype = torch.float)\n",
        "#y_train_tensor = torch.tensor(y_train, dtype = torch.float)\n",
        "#y_test_tensor = torch.tensor(y_test, dtype = torch.float)\n",
        "# Tensor time\n",
        "x_train_tensor = torch.tensor(x_train_std_np, dtype = torch.float)\n",
        "x_test_tensor = torch.tensor(x_test_std_np, dtype = torch.float)\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "y_test_tensor = torch.tensor(y_test)\n",
        "# Perform model creation\n",
        "# Define the double-layer neural network\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(31, 64),  # 31 input features, 64 neurons in the first hidden layer\n",
        "    nn.ReLU(),         # Activation function (you can choose others like nn.Tanh() or nn.Sigmoid())\n",
        "    nn.Linear(64, 1),   # 64 neurons in the first hidden layer, 1 output neuron for binary classification\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "#[parameter.shape for parameter in model.parameters()]\n",
        "# Create function to be called when performing training loop process\n",
        "def training(epochs,optimizer,model,loss,x_train,x_test,y_train,y_test):\n",
        "  for epoch in range(1,epochs+1):\n",
        "    model_train = model(x_train)\n",
        "    train_loss = loss_fn(model_train.squeeze(), y_train.float())\n",
        "    #train_loss = loss(model_train,y_train)\n",
        "    model_test = model(x_test)\n",
        "    test_loss = loss_fn(model_test.squeeze(), y_test.float())\n",
        "    #test_loss = loss(model_test,y_test)\n",
        "    # Perform optimization\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    # Dispay train and test losses at intervals of 1000\n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "      print((\"Iteration: \", epoch, \"Training Loss: \", train_loss.item(), \"Testing Loss: \", test_loss.item()))\n",
        "# Perform training on one layer with 32\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "loss_fn = nn.BCELoss()\n",
        "#loss_fn = nn.CrossEntropyLoss()\n",
        "training(epochs = 1500,\n",
        "         optimizer = optimizer,\n",
        "         model = model,\n",
        "         loss = loss_fn,\n",
        "         x_train = x_train_tensor,\n",
        "         x_test = x_test_tensor,\n",
        "         y_train = y_train_tensor,\n",
        "         y_test = y_test_tensor\n",
        ")\n",
        "\n",
        "# Apply the trained model to the test set for evaluation\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    y_pred = (model(x_test_tensor).squeeze() > 0.5).float()\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test_tensor, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test_tensor, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test_tensor, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_tensor, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp4NDUACuWbM",
        "outputId": "c7c09604-5b3c-4de4-a294-a344d54956e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "('Iteration: ', 1, 'Training Loss: ', 0.6801968812942505, 'Testing Loss: ', 0.681124746799469)\n",
            "('Iteration: ', 10, 'Training Loss: ', 0.6683349609375, 'Testing Loss: ', 0.6701939702033997)\n",
            "('Iteration: ', 20, 'Training Loss: ', 0.6569865942001343, 'Testing Loss: ', 0.659646213054657)\n",
            "('Iteration: ', 30, 'Training Loss: ', 0.646835207939148, 'Testing Loss: ', 0.6501069068908691)\n",
            "('Iteration: ', 40, 'Training Loss: ', 0.6374042630195618, 'Testing Loss: ', 0.6411463022232056)\n",
            "('Iteration: ', 50, 'Training Loss: ', 0.6283978819847107, 'Testing Loss: ', 0.6325032114982605)\n",
            "('Iteration: ', 60, 'Training Loss: ', 0.6196328401565552, 'Testing Loss: ', 0.6240221261978149)\n",
            "('Iteration: ', 70, 'Training Loss: ', 0.6109930872917175, 'Testing Loss: ', 0.6156008839607239)\n",
            "('Iteration: ', 80, 'Training Loss: ', 0.6024007201194763, 'Testing Loss: ', 0.6071730852127075)\n",
            "('Iteration: ', 90, 'Training Loss: ', 0.5938005447387695, 'Testing Loss: ', 0.5986952781677246)\n",
            "('Iteration: ', 100, 'Training Loss: ', 0.5851569175720215, 'Testing Loss: ', 0.5901399850845337)\n",
            "('Iteration: ', 110, 'Training Loss: ', 0.5764552354812622, 'Testing Loss: ', 0.5815007090568542)\n",
            "('Iteration: ', 120, 'Training Loss: ', 0.567683756351471, 'Testing Loss: ', 0.5727707147598267)\n",
            "('Iteration: ', 130, 'Training Loss: ', 0.5588358640670776, 'Testing Loss: ', 0.5639441609382629)\n",
            "('Iteration: ', 140, 'Training Loss: ', 0.5499176383018494, 'Testing Loss: ', 0.5550346970558167)\n",
            "('Iteration: ', 150, 'Training Loss: ', 0.5409260392189026, 'Testing Loss: ', 0.5460413098335266)\n",
            "('Iteration: ', 160, 'Training Loss: ', 0.5318693518638611, 'Testing Loss: ', 0.5369729995727539)\n",
            "('Iteration: ', 170, 'Training Loss: ', 0.5227655172348022, 'Testing Loss: ', 0.5278540849685669)\n",
            "('Iteration: ', 180, 'Training Loss: ', 0.5136340260505676, 'Testing Loss: ', 0.5187121033668518)\n",
            "('Iteration: ', 190, 'Training Loss: ', 0.5044866800308228, 'Testing Loss: ', 0.5095570683479309)\n",
            "('Iteration: ', 200, 'Training Loss: ', 0.4953504502773285, 'Testing Loss: ', 0.5004125237464905)\n",
            "('Iteration: ', 210, 'Training Loss: ', 0.4862436056137085, 'Testing Loss: ', 0.4913023114204407)\n",
            "('Iteration: ', 220, 'Training Loss: ', 0.47718456387519836, 'Testing Loss: ', 0.48224928975105286)\n",
            "('Iteration: ', 230, 'Training Loss: ', 0.46821075677871704, 'Testing Loss: ', 0.4732811748981476)\n",
            "('Iteration: ', 240, 'Training Loss: ', 0.45934224128723145, 'Testing Loss: ', 0.4644217789173126)\n",
            "('Iteration: ', 250, 'Training Loss: ', 0.45059749484062195, 'Testing Loss: ', 0.4556907117366791)\n",
            "('Iteration: ', 260, 'Training Loss: ', 0.44199225306510925, 'Testing Loss: ', 0.44710347056388855)\n",
            "('Iteration: ', 270, 'Training Loss: ', 0.43354472517967224, 'Testing Loss: ', 0.43867433071136475)\n",
            "('Iteration: ', 280, 'Training Loss: ', 0.42528006434440613, 'Testing Loss: ', 0.4304327368736267)\n",
            "('Iteration: ', 290, 'Training Loss: ', 0.417214035987854, 'Testing Loss: ', 0.4223966598510742)\n",
            "('Iteration: ', 300, 'Training Loss: ', 0.40936335921287537, 'Testing Loss: ', 0.4145776629447937)\n",
            "('Iteration: ', 310, 'Training Loss: ', 0.4017384350299835, 'Testing Loss: ', 0.4069896936416626)\n",
            "('Iteration: ', 320, 'Training Loss: ', 0.3943454623222351, 'Testing Loss: ', 0.3996351361274719)\n",
            "('Iteration: ', 330, 'Training Loss: ', 0.3871855139732361, 'Testing Loss: ', 0.39251500368118286)\n",
            "('Iteration: ', 340, 'Training Loss: ', 0.3802648186683655, 'Testing Loss: ', 0.38563263416290283)\n",
            "('Iteration: ', 350, 'Training Loss: ', 0.37358614802360535, 'Testing Loss: ', 0.37899065017700195)\n",
            "('Iteration: ', 360, 'Training Loss: ', 0.36714398860931396, 'Testing Loss: ', 0.3725849390029907)\n",
            "('Iteration: ', 370, 'Training Loss: ', 0.36094069480895996, 'Testing Loss: ', 0.36641472578048706)\n",
            "('Iteration: ', 380, 'Training Loss: ', 0.3549739420413971, 'Testing Loss: ', 0.360480397939682)\n",
            "('Iteration: ', 390, 'Training Loss: ', 0.3492409586906433, 'Testing Loss: ', 0.3547835052013397)\n",
            "('Iteration: ', 400, 'Training Loss: ', 0.34373342990875244, 'Testing Loss: ', 0.3493120074272156)\n",
            "('Iteration: ', 410, 'Training Loss: ', 0.3384477496147156, 'Testing Loss: ', 0.34406596422195435)\n",
            "('Iteration: ', 420, 'Training Loss: ', 0.33337557315826416, 'Testing Loss: ', 0.33903661370277405)\n",
            "('Iteration: ', 430, 'Training Loss: ', 0.3285115659236908, 'Testing Loss: ', 0.3342158794403076)\n",
            "('Iteration: ', 440, 'Training Loss: ', 0.32385140657424927, 'Testing Loss: ', 0.32960039377212524)\n",
            "('Iteration: ', 450, 'Training Loss: ', 0.3193839192390442, 'Testing Loss: ', 0.32517924904823303)\n",
            "('Iteration: ', 460, 'Training Loss: ', 0.3151082694530487, 'Testing Loss: ', 0.3209484815597534)\n",
            "('Iteration: ', 470, 'Training Loss: ', 0.3110128939151764, 'Testing Loss: ', 0.31689950823783875)\n",
            "('Iteration: ', 480, 'Training Loss: ', 0.3070926368236542, 'Testing Loss: ', 0.3130268454551697)\n",
            "('Iteration: ', 490, 'Training Loss: ', 0.30334052443504333, 'Testing Loss: ', 0.30932316184043884)\n",
            "('Iteration: ', 500, 'Training Loss: ', 0.2997455596923828, 'Testing Loss: ', 0.30577757954597473)\n",
            "('Iteration: ', 510, 'Training Loss: ', 0.29630255699157715, 'Testing Loss: ', 0.3023848235607147)\n",
            "('Iteration: ', 520, 'Training Loss: ', 0.29300472140312195, 'Testing Loss: ', 0.29913797974586487)\n",
            "('Iteration: ', 530, 'Training Loss: ', 0.2898465692996979, 'Testing Loss: ', 0.29603326320648193)\n",
            "('Iteration: ', 540, 'Training Loss: ', 0.2868231236934662, 'Testing Loss: ', 0.2930620312690735)\n",
            "('Iteration: ', 550, 'Training Loss: ', 0.2839265763759613, 'Testing Loss: ', 0.2902185022830963)\n",
            "('Iteration: ', 560, 'Training Loss: ', 0.28114819526672363, 'Testing Loss: ', 0.28749215602874756)\n",
            "('Iteration: ', 570, 'Training Loss: ', 0.27848339080810547, 'Testing Loss: ', 0.2848775386810303)\n",
            "('Iteration: ', 580, 'Training Loss: ', 0.2759254574775696, 'Testing Loss: ', 0.2823686897754669)\n",
            "('Iteration: ', 590, 'Training Loss: ', 0.2734687924385071, 'Testing Loss: ', 0.2799598276615143)\n",
            "('Iteration: ', 600, 'Training Loss: ', 0.2711082398891449, 'Testing Loss: ', 0.27764734625816345)\n",
            "('Iteration: ', 610, 'Training Loss: ', 0.26884233951568604, 'Testing Loss: ', 0.27542927861213684)\n",
            "('Iteration: ', 620, 'Training Loss: ', 0.2666657269001007, 'Testing Loss: ', 0.27329859137535095)\n",
            "('Iteration: ', 630, 'Training Loss: ', 0.2645736336708069, 'Testing Loss: ', 0.27125146985054016)\n",
            "('Iteration: ', 640, 'Training Loss: ', 0.26256272196769714, 'Testing Loss: ', 0.26928478479385376)\n",
            "('Iteration: ', 650, 'Training Loss: ', 0.26062849164009094, 'Testing Loss: ', 0.2673940360546112)\n",
            "('Iteration: ', 660, 'Training Loss: ', 0.2587680518627167, 'Testing Loss: ', 0.26557618379592896)\n",
            "('Iteration: ', 670, 'Training Loss: ', 0.256977379322052, 'Testing Loss: ', 0.2638273537158966)\n",
            "('Iteration: ', 680, 'Training Loss: ', 0.25525331497192383, 'Testing Loss: ', 0.26214495301246643)\n",
            "('Iteration: ', 690, 'Training Loss: ', 0.25359228253364563, 'Testing Loss: ', 0.2605256736278534)\n",
            "('Iteration: ', 700, 'Training Loss: ', 0.2519913911819458, 'Testing Loss: ', 0.2589658498764038)\n",
            "('Iteration: ', 710, 'Training Loss: ', 0.25044724345207214, 'Testing Loss: ', 0.25746282935142517)\n",
            "('Iteration: ', 720, 'Training Loss: ', 0.24895741045475006, 'Testing Loss: ', 0.25601479411125183)\n",
            "('Iteration: ', 730, 'Training Loss: ', 0.2475203573703766, 'Testing Loss: ', 0.2546192705631256)\n",
            "('Iteration: ', 740, 'Training Loss: ', 0.2461332231760025, 'Testing Loss: ', 0.25327348709106445)\n",
            "('Iteration: ', 750, 'Training Loss: ', 0.24479229748249054, 'Testing Loss: ', 0.25197282433509827)\n",
            "('Iteration: ', 760, 'Training Loss: ', 0.243496373295784, 'Testing Loss: ', 0.25071725249290466)\n",
            "('Iteration: ', 770, 'Training Loss: ', 0.24224364757537842, 'Testing Loss: ', 0.2495052069425583)\n",
            "('Iteration: ', 780, 'Training Loss: ', 0.24103207886219025, 'Testing Loss: ', 0.2483339160680771)\n",
            "('Iteration: ', 790, 'Training Loss: ', 0.23985961079597473, 'Testing Loss: ', 0.24720148742198944)\n",
            "('Iteration: ', 800, 'Training Loss: ', 0.2387235164642334, 'Testing Loss: ', 0.24610473215579987)\n",
            "('Iteration: ', 810, 'Training Loss: ', 0.23762281239032745, 'Testing Loss: ', 0.24504274129867554)\n",
            "('Iteration: ', 820, 'Training Loss: ', 0.23655591905117035, 'Testing Loss: ', 0.24401457607746124)\n",
            "('Iteration: ', 830, 'Training Loss: ', 0.23552165925502777, 'Testing Loss: ', 0.2430192232131958)\n",
            "('Iteration: ', 840, 'Training Loss: ', 0.2345181256532669, 'Testing Loss: ', 0.2420537769794464)\n",
            "('Iteration: ', 850, 'Training Loss: ', 0.23354361951351166, 'Testing Loss: ', 0.24111755192279816)\n",
            "('Iteration: ', 860, 'Training Loss: ', 0.23259755969047546, 'Testing Loss: ', 0.24020996689796448)\n",
            "('Iteration: ', 870, 'Training Loss: ', 0.2316770851612091, 'Testing Loss: ', 0.23932820558547974)\n",
            "('Iteration: ', 880, 'Training Loss: ', 0.23078279197216034, 'Testing Loss: ', 0.23847216367721558)\n",
            "('Iteration: ', 890, 'Training Loss: ', 0.22991320490837097, 'Testing Loss: ', 0.2376410812139511)\n",
            "('Iteration: ', 900, 'Training Loss: ', 0.2290678173303604, 'Testing Loss: ', 0.2368338257074356)\n",
            "('Iteration: ', 910, 'Training Loss: ', 0.22824552655220032, 'Testing Loss: ', 0.23604924976825714)\n",
            "('Iteration: ', 920, 'Training Loss: ', 0.22744573652744293, 'Testing Loss: ', 0.23528702557086945)\n",
            "('Iteration: ', 930, 'Training Loss: ', 0.22666646540164948, 'Testing Loss: ', 0.23454540967941284)\n",
            "('Iteration: ', 940, 'Training Loss: ', 0.22590748965740204, 'Testing Loss: ', 0.23382416367530823)\n",
            "('Iteration: ', 950, 'Training Loss: ', 0.22516776621341705, 'Testing Loss: ', 0.2331221103668213)\n",
            "('Iteration: ', 960, 'Training Loss: ', 0.22444738447666168, 'Testing Loss: ', 0.2324386090040207)\n",
            "('Iteration: ', 970, 'Training Loss: ', 0.22374436259269714, 'Testing Loss: ', 0.2317720502614975)\n",
            "('Iteration: ', 980, 'Training Loss: ', 0.22305847704410553, 'Testing Loss: ', 0.23112230002880096)\n",
            "('Iteration: ', 990, 'Training Loss: ', 0.22238953411579132, 'Testing Loss: ', 0.23048870265483856)\n",
            "('Iteration: ', 1000, 'Training Loss: ', 0.22173692286014557, 'Testing Loss: ', 0.22987093031406403)\n",
            "('Iteration: ', 1010, 'Training Loss: ', 0.2210998684167862, 'Testing Loss: ', 0.2292686402797699)\n",
            "('Iteration: ', 1020, 'Training Loss: ', 0.22047792375087738, 'Testing Loss: ', 0.2286810427904129)\n",
            "('Iteration: ', 1030, 'Training Loss: ', 0.21987037360668182, 'Testing Loss: ', 0.2281080186367035)\n",
            "('Iteration: ', 1040, 'Training Loss: ', 0.21927665174007416, 'Testing Loss: ', 0.22754916548728943)\n",
            "('Iteration: ', 1050, 'Training Loss: ', 0.21869634091854095, 'Testing Loss: ', 0.22700338065624237)\n",
            "('Iteration: ', 1060, 'Training Loss: ', 0.21812911331653595, 'Testing Loss: ', 0.22647057473659515)\n",
            "('Iteration: ', 1070, 'Training Loss: ', 0.2175743281841278, 'Testing Loss: ', 0.22595025599002838)\n",
            "('Iteration: ', 1080, 'Training Loss: ', 0.21703098714351654, 'Testing Loss: ', 0.22544166445732117)\n",
            "('Iteration: ', 1090, 'Training Loss: ', 0.21649843454360962, 'Testing Loss: ', 0.22494447231292725)\n",
            "('Iteration: ', 1100, 'Training Loss: ', 0.21597716212272644, 'Testing Loss: ', 0.2244585007429123)\n",
            "('Iteration: ', 1110, 'Training Loss: ', 0.21546657383441925, 'Testing Loss: ', 0.22398316860198975)\n",
            "('Iteration: ', 1120, 'Training Loss: ', 0.2149660289287567, 'Testing Loss: ', 0.22351805865764618)\n",
            "('Iteration: ', 1130, 'Training Loss: ', 0.2144753336906433, 'Testing Loss: ', 0.2230626791715622)\n",
            "('Iteration: ', 1140, 'Training Loss: ', 0.2139938622713089, 'Testing Loss: ', 0.22261592745780945)\n",
            "('Iteration: ', 1150, 'Training Loss: ', 0.21352174878120422, 'Testing Loss: ', 0.22217805683612823)\n",
            "('Iteration: ', 1160, 'Training Loss: ', 0.21305875480175018, 'Testing Loss: ', 0.22174927592277527)\n",
            "('Iteration: ', 1170, 'Training Loss: ', 0.2126045674085617, 'Testing Loss: ', 0.22132886946201324)\n",
            "('Iteration: ', 1180, 'Training Loss: ', 0.2121589630842209, 'Testing Loss: ', 0.22091683745384216)\n",
            "('Iteration: ', 1190, 'Training Loss: ', 0.2117215245962143, 'Testing Loss: ', 0.22051231563091278)\n",
            "('Iteration: ', 1200, 'Training Loss: ', 0.21129252016544342, 'Testing Loss: ', 0.22011607885360718)\n",
            "('Iteration: ', 1210, 'Training Loss: ', 0.21087177097797394, 'Testing Loss: ', 0.2197282761335373)\n",
            "('Iteration: ', 1220, 'Training Loss: ', 0.21045848727226257, 'Testing Loss: ', 0.21934761106967926)\n",
            "('Iteration: ', 1230, 'Training Loss: ', 0.21005253493785858, 'Testing Loss: ', 0.21897336840629578)\n",
            "('Iteration: ', 1240, 'Training Loss: ', 0.20965372025966644, 'Testing Loss: ', 0.21860629320144653)\n",
            "('Iteration: ', 1250, 'Training Loss: ', 0.2092617005109787, 'Testing Loss: ', 0.2182459682226181)\n",
            "('Iteration: ', 1260, 'Training Loss: ', 0.20887631177902222, 'Testing Loss: ', 0.21789196133613586)\n",
            "('Iteration: ', 1270, 'Training Loss: ', 0.20849713683128357, 'Testing Loss: ', 0.21754418313503265)\n",
            "('Iteration: ', 1280, 'Training Loss: ', 0.2081243395805359, 'Testing Loss: ', 0.21720248460769653)\n",
            "('Iteration: ', 1290, 'Training Loss: ', 0.20775814354419708, 'Testing Loss: ', 0.21686741709709167)\n",
            "('Iteration: ', 1300, 'Training Loss: ', 0.207397922873497, 'Testing Loss: ', 0.21653850376605988)\n",
            "('Iteration: ', 1310, 'Training Loss: ', 0.20704351365566254, 'Testing Loss: ', 0.21621529757976532)\n",
            "('Iteration: ', 1320, 'Training Loss: ', 0.2066948115825653, 'Testing Loss: ', 0.21589796245098114)\n",
            "('Iteration: ', 1330, 'Training Loss: ', 0.20635151863098145, 'Testing Loss: ', 0.21558672189712524)\n",
            "('Iteration: ', 1340, 'Training Loss: ', 0.206014022231102, 'Testing Loss: ', 0.2152814418077469)\n",
            "('Iteration: ', 1350, 'Training Loss: ', 0.20568203926086426, 'Testing Loss: ', 0.21498170495033264)\n",
            "('Iteration: ', 1360, 'Training Loss: ', 0.20535549521446228, 'Testing Loss: ', 0.2146868109703064)\n",
            "('Iteration: ', 1370, 'Training Loss: ', 0.20503370463848114, 'Testing Loss: ', 0.21439599990844727)\n",
            "('Iteration: ', 1380, 'Training Loss: ', 0.20471671223640442, 'Testing Loss: ', 0.21410955488681793)\n",
            "('Iteration: ', 1390, 'Training Loss: ', 0.20440459251403809, 'Testing Loss: ', 0.21382783353328705)\n",
            "('Iteration: ', 1400, 'Training Loss: ', 0.20409758388996124, 'Testing Loss: ', 0.21355114877223969)\n",
            "('Iteration: ', 1410, 'Training Loss: ', 0.2037954330444336, 'Testing Loss: ', 0.21327902376651764)\n",
            "('Iteration: ', 1420, 'Training Loss: ', 0.20349788665771484, 'Testing Loss: ', 0.21301133930683136)\n",
            "('Iteration: ', 1430, 'Training Loss: ', 0.20320501923561096, 'Testing Loss: ', 0.21274834871292114)\n",
            "('Iteration: ', 1440, 'Training Loss: ', 0.2029164731502533, 'Testing Loss: ', 0.21248963475227356)\n",
            "('Iteration: ', 1450, 'Training Loss: ', 0.2026321440935135, 'Testing Loss: ', 0.21223551034927368)\n",
            "('Iteration: ', 1460, 'Training Loss: ', 0.20235222578048706, 'Testing Loss: ', 0.21198539435863495)\n",
            "('Iteration: ', 1470, 'Training Loss: ', 0.20207631587982178, 'Testing Loss: ', 0.21173927187919617)\n",
            "('Iteration: ', 1480, 'Training Loss: ', 0.2018045037984848, 'Testing Loss: ', 0.21149732172489166)\n",
            "('Iteration: ', 1490, 'Training Loss: ', 0.20153653621673584, 'Testing Loss: ', 0.21125923097133636)\n",
            "('Iteration: ', 1500, 'Training Loss: ', 0.2012723833322525, 'Testing Loss: ', 0.21102477610111237)\n",
            "Accuracy: 0.9185888738127544\n",
            "Precision: 0.9124087591240876\n",
            "Recall: 0.9398496240601504\n",
            "Confusion Matrix:\n",
            " [[ 906  108]\n",
            " [  72 1125]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nick's Attempt at Double layer Neural Network with PyTorch\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from google.colab import drive\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import imageio\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/drive/MyDrive/PhishingCSV.csv\"\n",
        "phish = pd.read_csv(file_path)\n",
        "# Standardize the data\n",
        "#standard = StandardScaler()\n",
        "# Create a list of feature\n",
        "variable_list = ['having_IP_Address','URL_Length','Shortining_Service','having_At_Symbol','double_slash_redirecting','Prefix_Suffix','having_Sub_Domain','SSLfinal_State','Domain_registeration_length','Favicon','port','HTTPS_token','Request_URL','URL_of_Anchor','Links_in_tags','SFH','Submitting_to_email','Abnormal_URL','Redirect','on_mouseover','RightClick','popUpWidnow','Iframe','age_of_domain','DNSRecord','web_traffic','Page_Rank','Google_Index','Links_pointing_to_page','Statistical_report']\n",
        "\n",
        "# Extract features and labels\n",
        "x = phish.drop(['Result', 'id'], axis = 1)\n",
        "\n",
        "y = phish['Result']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "# Turn data set into seperate training and validation sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# Reshape the data to aviod problems with tensor creation\n",
        "#y_train = y_train.values.reshape(-1,1)\n",
        "#y_test = y_test.values.reshape(-1,1)\n",
        "# Stack the data\n",
        "x_train_std_np = np.c_[np.ones((len(y_train), 1)), x_train[variable_list]]\n",
        "#x_train_std_np = np.c_[np.ones((len(y_train), 1)), x_train[variable_list]]\n",
        "x_test_std_np = np.c_[np.ones((len(y_test), 1)), x_test[variable_list]]\n",
        "# Tensor time\n",
        "#x_train_tensor = torch.tensor(x_train_std_np, dtype = torch.float)\n",
        "#x_test_tensor = torch.tensor(x_test_std_np, dtype = torch.float)\n",
        "#y_train_tensor = torch.tensor(y_train, dtype = torch.float)\n",
        "#y_test_tensor = torch.tensor(y_test, dtype = torch.float)\n",
        "# Tensor time\n",
        "x_train_tensor = torch.tensor(x_train_std_np, dtype = torch.float)\n",
        "x_test_tensor = torch.tensor(x_test_std_np, dtype = torch.float)\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "y_test_tensor = torch.tensor(y_test)\n",
        "# Define the triple-layer neural network\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(31, 64),  # 31 input features, 64 neurons in the first hidden layer\n",
        "    nn.ReLU(),         # Activation function for the first hidden layer\n",
        "\n",
        "    nn.Linear(64, 32),  # 64 neurons in the first hidden layer, 32 neurons in the second hidden layer\n",
        "    nn.ReLU(),         # Activation function for the second hidden layer\n",
        "\n",
        "    nn.Linear(32, 1),   # 32 neurons in the second hidden layer, 1 output neuron for binary classification\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "#[parameter.shape for parameter in model.parameters()]\n",
        "# Create function to be called when performing training loop process\n",
        "def training(epochs,optimizer,model,loss,x_train,x_test,y_train,y_test):\n",
        "  for epoch in range(1,epochs+1):\n",
        "    model_train = model(x_train)\n",
        "    train_loss = loss_fn(model_train.squeeze(), y_train.float())\n",
        "    #train_loss = loss(model_train,y_train)\n",
        "    model_test = model(x_test)\n",
        "    test_loss = loss_fn(model_test.squeeze(), y_test.float())\n",
        "    #test_loss = loss(model_test,y_test)\n",
        "    # Perform optimization\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    # Dispay train and test losses at intervals of 1000\n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "      print((\"Iteration: \", epoch, \"Training Loss: \", train_loss.item(), \"Testing Loss: \", test_loss.item()))\n",
        "# Perform training on one layer with 32\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "loss_fn = nn.BCELoss()\n",
        "#loss_fn = nn.CrossEntropyLoss()\n",
        "training(epochs = 1500,\n",
        "         optimizer = optimizer,\n",
        "         model = model,\n",
        "         loss = loss_fn,\n",
        "         x_train = x_train_tensor,\n",
        "         x_test = x_test_tensor,\n",
        "         y_train = y_train_tensor,\n",
        "         y_test = y_test_tensor\n",
        ")\n",
        "\n",
        "# Apply the trained model to the test set for evaluation\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    y_pred = (model(x_test_tensor).squeeze() > 0.5).float()\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test_tensor, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test_tensor, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test_tensor, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_tensor, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bznCGWJ7w5yW",
        "outputId": "5e476259-f87f-46f2-f7b3-62bae0877931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "('Iteration: ', 1, 'Training Loss: ', 0.6892438530921936, 'Testing Loss: ', 0.6934092044830322)\n",
            "('Iteration: ', 10, 'Training Loss: ', 0.6879361867904663, 'Testing Loss: ', 0.6921582818031311)\n",
            "('Iteration: ', 20, 'Training Loss: ', 0.6864790916442871, 'Testing Loss: ', 0.6907670497894287)\n",
            "('Iteration: ', 30, 'Training Loss: ', 0.6850120425224304, 'Testing Loss: ', 0.6893699169158936)\n",
            "('Iteration: ', 40, 'Training Loss: ', 0.6835257411003113, 'Testing Loss: ', 0.6879522800445557)\n",
            "('Iteration: ', 50, 'Training Loss: ', 0.6820181608200073, 'Testing Loss: ', 0.6865178346633911)\n",
            "('Iteration: ', 60, 'Training Loss: ', 0.6804854273796082, 'Testing Loss: ', 0.6850588321685791)\n",
            "('Iteration: ', 70, 'Training Loss: ', 0.6789223551750183, 'Testing Loss: ', 0.6835735440254211)\n",
            "('Iteration: ', 80, 'Training Loss: ', 0.6773194074630737, 'Testing Loss: ', 0.682054877281189)\n",
            "('Iteration: ', 90, 'Training Loss: ', 0.6756786704063416, 'Testing Loss: ', 0.6805013418197632)\n",
            "('Iteration: ', 100, 'Training Loss: ', 0.6739941239356995, 'Testing Loss: ', 0.6789089441299438)\n",
            "('Iteration: ', 110, 'Training Loss: ', 0.6722681522369385, 'Testing Loss: ', 0.6772756576538086)\n",
            "('Iteration: ', 120, 'Training Loss: ', 0.6704869866371155, 'Testing Loss: ', 0.6755954027175903)\n",
            "('Iteration: ', 130, 'Training Loss: ', 0.6686494946479797, 'Testing Loss: ', 0.6738583445549011)\n",
            "('Iteration: ', 140, 'Training Loss: ', 0.6667543649673462, 'Testing Loss: ', 0.6720629334449768)\n",
            "('Iteration: ', 150, 'Training Loss: ', 0.664795458316803, 'Testing Loss: ', 0.6702091097831726)\n",
            "('Iteration: ', 160, 'Training Loss: ', 0.6627781987190247, 'Testing Loss: ', 0.6682919859886169)\n",
            "('Iteration: ', 170, 'Training Loss: ', 0.6606862545013428, 'Testing Loss: ', 0.6663002967834473)\n",
            "('Iteration: ', 180, 'Training Loss: ', 0.6585105657577515, 'Testing Loss: ', 0.664230227470398)\n",
            "('Iteration: ', 190, 'Training Loss: ', 0.6562422513961792, 'Testing Loss: ', 0.6620697379112244)\n",
            "('Iteration: ', 200, 'Training Loss: ', 0.6538777947425842, 'Testing Loss: ', 0.6598171591758728)\n",
            "('Iteration: ', 210, 'Training Loss: ', 0.6514055728912354, 'Testing Loss: ', 0.6574644446372986)\n",
            "('Iteration: ', 220, 'Training Loss: ', 0.648819625377655, 'Testing Loss: ', 0.6550025343894958)\n",
            "('Iteration: ', 230, 'Training Loss: ', 0.6461170315742493, 'Testing Loss: ', 0.6524275541305542)\n",
            "('Iteration: ', 240, 'Training Loss: ', 0.6432876586914062, 'Testing Loss: ', 0.6497290134429932)\n",
            "('Iteration: ', 250, 'Training Loss: ', 0.640317440032959, 'Testing Loss: ', 0.6468905210494995)\n",
            "('Iteration: ', 260, 'Training Loss: ', 0.6372052431106567, 'Testing Loss: ', 0.643914520740509)\n",
            "('Iteration: ', 270, 'Training Loss: ', 0.6339358687400818, 'Testing Loss: ', 0.6407831907272339)\n",
            "('Iteration: ', 280, 'Training Loss: ', 0.630508542060852, 'Testing Loss: ', 0.6374923586845398)\n",
            "('Iteration: ', 290, 'Training Loss: ', 0.6269093751907349, 'Testing Loss: ', 0.6340375542640686)\n",
            "('Iteration: ', 300, 'Training Loss: ', 0.6231380105018616, 'Testing Loss: ', 0.6304132342338562)\n",
            "('Iteration: ', 310, 'Training Loss: ', 0.6191781163215637, 'Testing Loss: ', 0.6266056895256042)\n",
            "('Iteration: ', 320, 'Training Loss: ', 0.6150145530700684, 'Testing Loss: ', 0.6226019859313965)\n",
            "('Iteration: ', 330, 'Training Loss: ', 0.6106363534927368, 'Testing Loss: ', 0.6183944940567017)\n",
            "('Iteration: ', 340, 'Training Loss: ', 0.6060475707054138, 'Testing Loss: ', 0.6139811873435974)\n",
            "('Iteration: ', 350, 'Training Loss: ', 0.6012410521507263, 'Testing Loss: ', 0.6093453168869019)\n",
            "('Iteration: ', 360, 'Training Loss: ', 0.5962046384811401, 'Testing Loss: ', 0.6044813394546509)\n",
            "('Iteration: ', 370, 'Training Loss: ', 0.5909342169761658, 'Testing Loss: ', 0.5993881821632385)\n",
            "('Iteration: ', 380, 'Training Loss: ', 0.5854275822639465, 'Testing Loss: ', 0.5940641760826111)\n",
            "('Iteration: ', 390, 'Training Loss: ', 0.5796785950660706, 'Testing Loss: ', 0.5885025858879089)\n",
            "('Iteration: ', 400, 'Training Loss: ', 0.5736823081970215, 'Testing Loss: ', 0.5826985836029053)\n",
            "('Iteration: ', 410, 'Training Loss: ', 0.5674341320991516, 'Testing Loss: ', 0.576643168926239)\n",
            "('Iteration: ', 420, 'Training Loss: ', 0.5609300136566162, 'Testing Loss: ', 0.5703241229057312)\n",
            "('Iteration: ', 430, 'Training Loss: ', 0.5541694760322571, 'Testing Loss: ', 0.5637475848197937)\n",
            "('Iteration: ', 440, 'Training Loss: ', 0.5471569299697876, 'Testing Loss: ', 0.5569202899932861)\n",
            "('Iteration: ', 450, 'Training Loss: ', 0.5398945808410645, 'Testing Loss: ', 0.5498418807983398)\n",
            "('Iteration: ', 460, 'Training Loss: ', 0.5323951244354248, 'Testing Loss: ', 0.5425280332565308)\n",
            "('Iteration: ', 470, 'Training Loss: ', 0.5246646404266357, 'Testing Loss: ', 0.5349817276000977)\n",
            "('Iteration: ', 480, 'Training Loss: ', 0.5167149901390076, 'Testing Loss: ', 0.5272178053855896)\n",
            "('Iteration: ', 490, 'Training Loss: ', 0.5085529685020447, 'Testing Loss: ', 0.5192410945892334)\n",
            "('Iteration: ', 500, 'Training Loss: ', 0.5001941323280334, 'Testing Loss: ', 0.5110618472099304)\n",
            "('Iteration: ', 510, 'Training Loss: ', 0.49166277050971985, 'Testing Loss: ', 0.5027105808258057)\n",
            "('Iteration: ', 520, 'Training Loss: ', 0.48299381136894226, 'Testing Loss: ', 0.49422183632850647)\n",
            "('Iteration: ', 530, 'Training Loss: ', 0.47420722246170044, 'Testing Loss: ', 0.4856058657169342)\n",
            "('Iteration: ', 540, 'Training Loss: ', 0.4653330147266388, 'Testing Loss: ', 0.4769003987312317)\n",
            "('Iteration: ', 550, 'Training Loss: ', 0.45638713240623474, 'Testing Loss: ', 0.46812739968299866)\n",
            "('Iteration: ', 560, 'Training Loss: ', 0.4473995268344879, 'Testing Loss: ', 0.4593164026737213)\n",
            "('Iteration: ', 570, 'Training Loss: ', 0.43840140104293823, 'Testing Loss: ', 0.45049819350242615)\n",
            "('Iteration: ', 580, 'Training Loss: ', 0.4294322729110718, 'Testing Loss: ', 0.44170108437538147)\n",
            "('Iteration: ', 590, 'Training Loss: ', 0.42051756381988525, 'Testing Loss: ', 0.4329541325569153)\n",
            "('Iteration: ', 600, 'Training Loss: ', 0.41169068217277527, 'Testing Loss: ', 0.42429423332214355)\n",
            "('Iteration: ', 610, 'Training Loss: ', 0.40298083424568176, 'Testing Loss: ', 0.41574904322624207)\n",
            "('Iteration: ', 620, 'Training Loss: ', 0.3944237232208252, 'Testing Loss: ', 0.4073551595211029)\n",
            "('Iteration: ', 630, 'Training Loss: ', 0.38604113459587097, 'Testing Loss: ', 0.3991242051124573)\n",
            "('Iteration: ', 640, 'Training Loss: ', 0.37786027789115906, 'Testing Loss: ', 0.3910881578922272)\n",
            "('Iteration: ', 650, 'Training Loss: ', 0.36989331245422363, 'Testing Loss: ', 0.3832615315914154)\n",
            "('Iteration: ', 660, 'Training Loss: ', 0.3621675968170166, 'Testing Loss: ', 0.37567394971847534)\n",
            "('Iteration: ', 670, 'Training Loss: ', 0.3546946942806244, 'Testing Loss: ', 0.3683377802371979)\n",
            "('Iteration: ', 680, 'Training Loss: ', 0.3474820554256439, 'Testing Loss: ', 0.36126092076301575)\n",
            "('Iteration: ', 690, 'Training Loss: ', 0.3405424654483795, 'Testing Loss: ', 0.35445505380630493)\n",
            "('Iteration: ', 700, 'Training Loss: ', 0.3338789939880371, 'Testing Loss: ', 0.34792235493659973)\n",
            "('Iteration: ', 710, 'Training Loss: ', 0.327496200799942, 'Testing Loss: ', 0.341661274433136)\n",
            "('Iteration: ', 720, 'Training Loss: ', 0.3213993310928345, 'Testing Loss: ', 0.33567893505096436)\n",
            "('Iteration: ', 730, 'Training Loss: ', 0.3155798614025116, 'Testing Loss: ', 0.32997050881385803)\n",
            "('Iteration: ', 740, 'Training Loss: ', 0.31003332138061523, 'Testing Loss: ', 0.32453152537345886)\n",
            "('Iteration: ', 750, 'Training Loss: ', 0.3047564625740051, 'Testing Loss: ', 0.3193569779396057)\n",
            "('Iteration: ', 760, 'Training Loss: ', 0.29974234104156494, 'Testing Loss: ', 0.3144403100013733)\n",
            "('Iteration: ', 770, 'Training Loss: ', 0.2949846088886261, 'Testing Loss: ', 0.30977383255958557)\n",
            "('Iteration: ', 780, 'Training Loss: ', 0.2904677987098694, 'Testing Loss: ', 0.3053422272205353)\n",
            "('Iteration: ', 790, 'Training Loss: ', 0.28618308901786804, 'Testing Loss: ', 0.3011380136013031)\n",
            "('Iteration: ', 800, 'Training Loss: ', 0.2821181118488312, 'Testing Loss: ', 0.297146201133728)\n",
            "('Iteration: ', 810, 'Training Loss: ', 0.27826735377311707, 'Testing Loss: ', 0.29336661100387573)\n",
            "('Iteration: ', 820, 'Training Loss: ', 0.27461862564086914, 'Testing Loss: ', 0.28978773951530457)\n",
            "('Iteration: ', 830, 'Training Loss: ', 0.2711617052555084, 'Testing Loss: ', 0.2863921821117401)\n",
            "('Iteration: ', 840, 'Training Loss: ', 0.2678825557231903, 'Testing Loss: ', 0.2831682860851288)\n",
            "('Iteration: ', 850, 'Training Loss: ', 0.26477721333503723, 'Testing Loss: ', 0.28011250495910645)\n",
            "('Iteration: ', 860, 'Training Loss: ', 0.26183634996414185, 'Testing Loss: ', 0.27721288800239563)\n",
            "('Iteration: ', 870, 'Training Loss: ', 0.2590460479259491, 'Testing Loss: ', 0.2744560241699219)\n",
            "('Iteration: ', 880, 'Training Loss: ', 0.25640177726745605, 'Testing Loss: ', 0.27183640003204346)\n",
            "('Iteration: ', 890, 'Training Loss: ', 0.2538962662220001, 'Testing Loss: ', 0.26934799551963806)\n",
            "('Iteration: ', 900, 'Training Loss: ', 0.25151512026786804, 'Testing Loss: ', 0.26698458194732666)\n",
            "('Iteration: ', 910, 'Training Loss: ', 0.24925482273101807, 'Testing Loss: ', 0.2647396922111511)\n",
            "('Iteration: ', 920, 'Training Loss: ', 0.24710634350776672, 'Testing Loss: ', 0.2626018226146698)\n",
            "('Iteration: ', 930, 'Training Loss: ', 0.2450636625289917, 'Testing Loss: ', 0.2605670988559723)\n",
            "('Iteration: ', 940, 'Training Loss: ', 0.24311953783035278, 'Testing Loss: ', 0.2586252987384796)\n",
            "('Iteration: ', 950, 'Training Loss: ', 0.2412678301334381, 'Testing Loss: ', 0.25677332282066345)\n",
            "('Iteration: ', 960, 'Training Loss: ', 0.23949754238128662, 'Testing Loss: ', 0.255003958940506)\n",
            "('Iteration: ', 970, 'Training Loss: ', 0.23780934512615204, 'Testing Loss: ', 0.25331416726112366)\n",
            "('Iteration: ', 980, 'Training Loss: ', 0.23619534075260162, 'Testing Loss: ', 0.2516918182373047)\n",
            "('Iteration: ', 990, 'Training Loss: ', 0.234651580452919, 'Testing Loss: ', 0.25013670325279236)\n",
            "('Iteration: ', 1000, 'Training Loss: ', 0.23317483067512512, 'Testing Loss: ', 0.24864725768566132)\n",
            "('Iteration: ', 1010, 'Training Loss: ', 0.2317640632390976, 'Testing Loss: ', 0.2472223937511444)\n",
            "('Iteration: ', 1020, 'Training Loss: ', 0.230412095785141, 'Testing Loss: ', 0.24585554003715515)\n",
            "('Iteration: ', 1030, 'Training Loss: ', 0.2291155457496643, 'Testing Loss: ', 0.2445429116487503)\n",
            "('Iteration: ', 1040, 'Training Loss: ', 0.2278718203306198, 'Testing Loss: ', 0.2432827353477478)\n",
            "('Iteration: ', 1050, 'Training Loss: ', 0.22667796909809113, 'Testing Loss: ', 0.24207203090190887)\n",
            "('Iteration: ', 1060, 'Training Loss: ', 0.22553111612796783, 'Testing Loss: ', 0.2409077286720276)\n",
            "('Iteration: ', 1070, 'Training Loss: ', 0.22442734241485596, 'Testing Loss: ', 0.23978611826896667)\n",
            "('Iteration: ', 1080, 'Training Loss: ', 0.2233659327030182, 'Testing Loss: ', 0.23870617151260376)\n",
            "('Iteration: ', 1090, 'Training Loss: ', 0.22234447300434113, 'Testing Loss: ', 0.2376655638217926)\n",
            "('Iteration: ', 1100, 'Training Loss: ', 0.2213587760925293, 'Testing Loss: ', 0.23665787279605865)\n",
            "('Iteration: ', 1110, 'Training Loss: ', 0.22040706872940063, 'Testing Loss: ', 0.23568271100521088)\n",
            "('Iteration: ', 1120, 'Training Loss: ', 0.21948885917663574, 'Testing Loss: ', 0.2347395271062851)\n",
            "('Iteration: ', 1130, 'Training Loss: ', 0.21860158443450928, 'Testing Loss: ', 0.23382726311683655)\n",
            "('Iteration: ', 1140, 'Training Loss: ', 0.21774117648601532, 'Testing Loss: ', 0.23294298350811005)\n",
            "('Iteration: ', 1150, 'Training Loss: ', 0.21690812706947327, 'Testing Loss: ', 0.23208622634410858)\n",
            "('Iteration: ', 1160, 'Training Loss: ', 0.21609829366207123, 'Testing Loss: ', 0.23125408589839935)\n",
            "('Iteration: ', 1170, 'Training Loss: ', 0.21531175076961517, 'Testing Loss: ', 0.2304452806711197)\n",
            "('Iteration: ', 1180, 'Training Loss: ', 0.21454282104969025, 'Testing Loss: ', 0.22965772449970245)\n",
            "('Iteration: ', 1190, 'Training Loss: ', 0.21379630267620087, 'Testing Loss: ', 0.22889138758182526)\n",
            "('Iteration: ', 1200, 'Training Loss: ', 0.21307258307933807, 'Testing Loss: ', 0.22814632952213287)\n",
            "('Iteration: ', 1210, 'Training Loss: ', 0.21236969530582428, 'Testing Loss: ', 0.22742217779159546)\n",
            "('Iteration: ', 1220, 'Training Loss: ', 0.21168553829193115, 'Testing Loss: ', 0.22671663761138916)\n",
            "('Iteration: ', 1230, 'Training Loss: ', 0.21101883053779602, 'Testing Loss: ', 0.22603067755699158)\n",
            "('Iteration: ', 1240, 'Training Loss: ', 0.2103692889213562, 'Testing Loss: ', 0.22536002099514008)\n",
            "('Iteration: ', 1250, 'Training Loss: ', 0.209736168384552, 'Testing Loss: ', 0.22470493614673615)\n",
            "('Iteration: ', 1260, 'Training Loss: ', 0.209119975566864, 'Testing Loss: ', 0.2240675836801529)\n",
            "('Iteration: ', 1270, 'Training Loss: ', 0.20851972699165344, 'Testing Loss: ', 0.22344814240932465)\n",
            "('Iteration: ', 1280, 'Training Loss: ', 0.20793516933918, 'Testing Loss: ', 0.22284509241580963)\n",
            "('Iteration: ', 1290, 'Training Loss: ', 0.20736554265022278, 'Testing Loss: ', 0.2222588062286377)\n",
            "('Iteration: ', 1300, 'Training Loss: ', 0.20680905878543854, 'Testing Loss: ', 0.22168642282485962)\n",
            "('Iteration: ', 1310, 'Training Loss: ', 0.2062663435935974, 'Testing Loss: ', 0.22112871706485748)\n",
            "('Iteration: ', 1320, 'Training Loss: ', 0.20573727786540985, 'Testing Loss: ', 0.2205841839313507)\n",
            "('Iteration: ', 1330, 'Training Loss: ', 0.2052207887172699, 'Testing Loss: ', 0.22005246579647064)\n",
            "('Iteration: ', 1340, 'Training Loss: ', 0.20471589267253876, 'Testing Loss: ', 0.21953116357326508)\n",
            "('Iteration: ', 1350, 'Training Loss: ', 0.2042236626148224, 'Testing Loss: ', 0.2190214991569519)\n",
            "('Iteration: ', 1360, 'Training Loss: ', 0.20374314486980438, 'Testing Loss: ', 0.21852517127990723)\n",
            "('Iteration: ', 1370, 'Training Loss: ', 0.20327438414096832, 'Testing Loss: ', 0.21804124116897583)\n",
            "('Iteration: ', 1380, 'Training Loss: ', 0.20281541347503662, 'Testing Loss: ', 0.21756862103939056)\n",
            "('Iteration: ', 1390, 'Training Loss: ', 0.20236629247665405, 'Testing Loss: ', 0.21710623800754547)\n",
            "('Iteration: ', 1400, 'Training Loss: ', 0.20192670822143555, 'Testing Loss: ', 0.21665416657924652)\n",
            "('Iteration: ', 1410, 'Training Loss: ', 0.20149604976177216, 'Testing Loss: ', 0.21621288359165192)\n",
            "('Iteration: ', 1420, 'Training Loss: ', 0.2010742425918579, 'Testing Loss: ', 0.21578161418437958)\n",
            "('Iteration: ', 1430, 'Training Loss: ', 0.20066116750240326, 'Testing Loss: ', 0.2153601199388504)\n",
            "('Iteration: ', 1440, 'Training Loss: ', 0.20025528967380524, 'Testing Loss: ', 0.2149476706981659)\n",
            "('Iteration: ', 1450, 'Training Loss: ', 0.1998576521873474, 'Testing Loss: ', 0.21454299986362457)\n",
            "('Iteration: ', 1460, 'Training Loss: ', 0.19946826994419098, 'Testing Loss: ', 0.21414564549922943)\n",
            "('Iteration: ', 1470, 'Training Loss: ', 0.19908563792705536, 'Testing Loss: ', 0.21375496685504913)\n",
            "('Iteration: ', 1480, 'Training Loss: ', 0.19870983064174652, 'Testing Loss: ', 0.21336998045444489)\n",
            "('Iteration: ', 1490, 'Training Loss: ', 0.1983414888381958, 'Testing Loss: ', 0.21299201250076294)\n",
            "('Iteration: ', 1500, 'Training Loss: ', 0.19798053801059723, 'Testing Loss: ', 0.21262110769748688)\n",
            "Accuracy: 0.9172320217096337\n",
            "Precision: 0.911525974025974\n",
            "Recall: 0.9381787802840434\n",
            "Confusion Matrix:\n",
            " [[ 905  109]\n",
            " [  74 1123]]\n"
          ]
        }
      ]
    }
  ]
}