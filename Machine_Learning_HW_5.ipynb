{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOAPpyxsEIPuwi8cV5/DXuF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrussell93/prework-about-me/blob/master/Machine_Learning_HW_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZa5Ap0YPGwy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5281ba91-646c-4c84-d164-edc5c1e5c1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD 1: lr = 0.1\n",
            "Epoch 500, Loss 2.291242\n",
            "Epoch 1000, Loss 2.208752\n",
            "Epoch 1500, Loss 2.160197\n",
            "Epoch 2000, Loss 2.131616\n",
            "Epoch 2500, Loss 2.114792\n",
            "Epoch 3000, Loss 2.104889\n",
            "Epoch 3500, Loss 2.099058\n",
            "Epoch 4000, Loss 2.095627\n",
            "Epoch 4500, Loss 2.093609\n",
            "Epoch 5000, Loss 2.092421\n",
            "Adam 1: lr = 0.1\n",
            "Epoch 500, Loss 2.295946\n",
            "Epoch 1000, Loss 2.245633\n",
            "Epoch 1500, Loss 2.200540\n",
            "Epoch 2000, Loss 2.158940\n",
            "Epoch 2500, Loss 2.126793\n",
            "Epoch 3000, Loss 2.106316\n",
            "Epoch 3500, Loss 2.095942\n",
            "Epoch 4000, Loss 2.091979\n",
            "Epoch 4500, Loss 2.090919\n",
            "Epoch 5000, Loss 2.090737\n",
            "SGD 2: lr = 0.01\n",
            "Epoch 500, Loss 3.120573\n",
            "Epoch 1000, Loss 2.420525\n",
            "Epoch 1500, Loss 2.381832\n",
            "Epoch 2000, Loss 2.366083\n",
            "Epoch 2500, Loss 2.351849\n",
            "Epoch 3000, Loss 2.338375\n",
            "Epoch 3500, Loss 2.325597\n",
            "Epoch 4000, Loss 2.313477\n",
            "Epoch 4500, Loss 2.301984\n",
            "Epoch 5000, Loss 2.291083\n",
            "Adam 2: lr = 0.01\n",
            "Epoch 500, Loss 23.904425\n",
            "Epoch 1000, Loss 15.672408\n",
            "Epoch 1500, Loss 9.210190\n",
            "Epoch 2000, Loss 5.163225\n",
            "Epoch 2500, Loss 3.198329\n",
            "Epoch 3000, Loss 2.486622\n",
            "Epoch 3500, Loss 2.305223\n",
            "Epoch 4000, Loss 2.270973\n",
            "Epoch 4500, Loss 2.259895\n",
            "Epoch 5000, Loss 2.249112\n",
            "SGD 3: lr = 0.001\n",
            "Epoch 500, Loss 17.421333\n",
            "Epoch 1000, Loss 13.073258\n",
            "Epoch 1500, Loss 10.009933\n",
            "Epoch 2000, Loss 7.827079\n",
            "Epoch 2500, Loss 6.271463\n",
            "Epoch 3000, Loss 5.162714\n",
            "Epoch 3500, Loss 4.372326\n",
            "Epoch 4000, Loss 3.808746\n",
            "Epoch 4500, Loss 3.406753\n",
            "Epoch 5000, Loss 3.119881\n",
            "Adam 3: lr = 0.001\n",
            "Epoch 500, Loss 99.486519\n",
            "Epoch 1000, Loss 76.216881\n",
            "Epoch 1500, Loss 58.758461\n",
            "Epoch 2000, Loss 46.067604\n",
            "Epoch 2500, Loss 37.217392\n",
            "Epoch 3000, Loss 31.339947\n",
            "Epoch 3500, Loss 27.590132\n",
            "Epoch 4000, Loss 25.153521\n",
            "Epoch 4500, Loss 23.326702\n",
            "Epoch 5000, Loss 21.646196\n",
            "SGD 4: lr = 0.0001\n",
            "Epoch 500, Loss 72.140999\n",
            "Epoch 1000, Loss 44.976524\n",
            "Epoch 1500, Loss 32.024796\n",
            "Epoch 2000, Loss 25.683241\n",
            "Epoch 2500, Loss 22.421692\n",
            "Epoch 3000, Loss 20.600744\n",
            "Epoch 3500, Loss 19.458881\n",
            "Epoch 4000, Loss 18.642195\n",
            "Epoch 4500, Loss 17.986025\n",
            "Epoch 5000, Loss 17.413618\n",
            "Adam 4: lr = 0.0001\n",
            "Epoch 500, Loss 126.355568\n",
            "Epoch 1000, Loss 123.056374\n",
            "Epoch 1500, Loss 119.831276\n",
            "Epoch 2000, Loss 116.675774\n",
            "Epoch 2500, Loss 113.586372\n",
            "Epoch 3000, Loss 110.559807\n",
            "Epoch 3500, Loss 107.593781\n",
            "Epoch 4000, Loss 104.686722\n",
            "Epoch 4500, Loss 101.837013\n",
            "Epoch 5000, Loss 99.043671\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.4882, 1.4894, 0.4859], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Machine Learning HW_5\n",
        "\n",
        "# Problem 1\n",
        "# Part a\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
        "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0,\n",
        "                    8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
        "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9,\n",
        "                    33.9, 21.8, 48.4, 60.4, 68.4])\n",
        "\n",
        "# Normalize t_u\n",
        "t_un = 0.02 * t_u\n",
        "def model(t_u, w1, w2, b):\n",
        "    return w2 * t_u ** 2 + w1 * t_u + b\n",
        "def loss_fn(t_p, t_c):\n",
        "    squared_diffs = (t_p - t_c)**2\n",
        "    return squared_diffs.mean()\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate_test = 0.1\n",
        "optimizer = optim.SGD([params], lr=learning_rate_test)\n",
        "t_p_test_SGD = model(t_un, *params)\n",
        "loss_test = loss_fn(model(t_un, *params), t_c)\n",
        "loss_test.backward()\n",
        "params.grad\n",
        "\n",
        "learning_rate_test = 0.1\n",
        "optimizer = optim.SGD([params], lr=learning_rate_test)\n",
        "t_p_test_SGD = model(t_un, *params)\n",
        "loss_test_SGD = loss_fn(t_p_test_SGD, t_c)\n",
        "optimizer.zero_grad()\n",
        "loss_test_SGD.backward()\n",
        "optimizer.step()\n",
        "params\n",
        "\n",
        "# Create training loop\n",
        "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    if params.grad is not None:\n",
        "      params.grad.zero_()\n",
        "    t_p = model(t_u, *params)\n",
        "    loss = loss_fn(t_p, t_c)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 500 == 0:\n",
        "      print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "  return params\n",
        "\n",
        "\n",
        "# First SGD optimizer at lr = 0.1\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate_test_1 = 0.1\n",
        "optimizer_test_1_SGD = optim.SGD([params], lr=learning_rate_test_1)# <1>\n",
        "print('SGD 1: lr = 0.1')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_test_1_SGD,\n",
        "    params = params, # <1>\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)\n",
        "\n",
        "# First ADAM optimizer at lr = 0.1\n",
        "# After running Adam_1 produced the lowest losses\n",
        "# at\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate_test_1 = 0.1\n",
        "optimizer_test_1_adam = optim.Adam([params], lr=learning_rate_test_1)# <1>\n",
        "print('Adam 1: lr = 0.1')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_test_1_adam,\n",
        "    params = params, # <1>\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)\n",
        "\n",
        "# Second SGD optimizer at lr = 0.01\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate_test_2 = 0.01\n",
        "optimizer_test_2_SGD = optim.SGD([params], lr=learning_rate_test_2)# <1>\n",
        "print('SGD 2: lr = 0.01')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_test_2_SGD,\n",
        "    params = params, # <1>\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)\n",
        "\n",
        "# Second ADAM optimizer at lr = 0.01\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate_test_2 = 0.01\n",
        "optimizer_test_2_adam = optim.Adam([params], lr=learning_rate_test_2)# <1>\n",
        "print('Adam 2: lr = 0.01')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_test_2_adam,\n",
        "    params = params, # <1>\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)\n",
        "\n",
        "# Third SGD optimizer at lr = 0.001\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate_test_3 = 0.001\n",
        "optimizer_test_3_SGD = optim.SGD([params], lr=learning_rate_test_3)# <1>\n",
        "print('SGD 3: lr = 0.001')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_test_3_SGD,\n",
        "    params = params, # <1>\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)\n",
        "\n",
        "# Third ADAM optimizer at lr = 0.001\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate_test_3 = 0.001\n",
        "optimizer_test_3_adam = optim.Adam([params], lr=learning_rate_test_3)# <1>\n",
        "print('Adam 3: lr = 0.001')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_test_3_adam,\n",
        "    params = params, # <1>\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)\n",
        "\n",
        "# Forth SGD optimizer at lr = 0.0001\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate_test_4 = 0.0001\n",
        "optimizer_test_4_SGD = optim.SGD([params], lr=learning_rate_test_4)# <1>\n",
        "print('SGD 4: lr = 0.0001')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_test_4_SGD,\n",
        "    params = params, # <1>\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)\n",
        "\n",
        "# Forth ADAM optimizer at lr = 0.0001\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate_test_4 = 0.0001\n",
        "optimizer_test_4_adam = optim.Adam([params], lr=learning_rate_test_4)# <1>\n",
        "print('Adam 4: lr = 0.0001')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_test_4_adam,\n",
        "    params = params, # <1>\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2\n",
        "# part a\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch.optim as optim\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/Housing.csv'\n",
        "housing = (pd.read_csv(file_path))\n",
        "housing.head()\n",
        "\n",
        "#cols = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "#variable_list = ['parking']\n",
        "# Create the list of variables within housing\n",
        "variable_list = ['area','bedrooms','bathrooms','stories','parking','mainroad','guestroom','basement','hotwaterheating','airconditioning','prefarea','furnishingstatus']\n",
        "#Turn Yes and No into 1 and 0 only on variables in variable_change_list\n",
        "variable_change_list = ['basement','airconditioning','prefarea','furnishingstatus','mainroad','guestroom','hotwaterheating']\n",
        "def data_map(x):\n",
        "    return x.map({'yes': 1, 'no': 0, 'unfurnished': -1, 'semi-furnished': 0, 'furnished': 1})\n",
        "# Apply the numerical conversions made when mapping\n",
        "housing[variable_change_list] = housing[variable_change_list].apply(data_map)\n",
        "housing.head()\n",
        "\n",
        "# Standardize the data\n",
        "standard = StandardScaler()\n",
        "housing_std = housing\n",
        "housing_std[variable_list] = standard.fit_transform(housing_std[variable_list])\n",
        "\n",
        "cols = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "# Seperate data into inputs and output\n",
        "x = housing_std[cols]\n",
        "y = housing_std['price']\n",
        "# Split data into training and test sets (80% training, 20% testing)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Stack input\n",
        "x_train_np = np.c_[np.ones((len(y_train), 1)), x_train[cols]]\n",
        "x_test_np = np.c_[np.ones((len(y_test), 1)), x_test[cols]]\n",
        "\n",
        "# Create tensor list\n",
        "x_train_tensor = torch.tensor(x_train_np)\n",
        "x_test_tensor = torch.tensor(x_test_np)\n",
        "y_train_tensor = torch.tensor(y_train.values)\n",
        "y_test_tensor = torch.tensor(y_test.values)\n",
        "\n",
        "y_train_mean = y_train_tensor.float().mean()\n",
        "y_train = y_train.std()\n",
        "y_train_std_tensor = (y_train_tensor - y_train_mean) / y_train\n",
        "\n",
        "y_test_mean = y_train_tensor.float().mean()\n",
        "y_test = y_test.std()\n",
        "y_test_std_tensor = (y_test_tensor - y_test_mean) / y_test\n",
        "\n",
        "def model(t_u, w5, w4, w3, w2, w, b):\n",
        "  return w5*t_u**5 + w4*t_u**4 + w3*t_u**3 + w2*t_u**2 + w*t_u**1 + b\n",
        "\n",
        "def loss_fn(t_p, t_c):\n",
        "  squared_diff = (t_p- - t_c) ** 2\n",
        "  return squared_diff.mean()\n",
        "\n",
        "# Create the training loop\n",
        "def training_loop(n_epochs, optimizer, params, t_u_train, t_c_train, t_u_test, t_c_test):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    if params.grad is not None:\n",
        "      params.grad.zero_()\n",
        "    t_p_train = model(t_u_train, *params)\n",
        "    loss_train = loss_fn(t_p_train.transpose(0, 1), t_c_train)\n",
        "    t_p_test = model(t_u_test, *params)\n",
        "    loss_test = loss_fn(t_p_test.transpose(0, 1), t_c_test)\n",
        "    optimizer.zero_grad()\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch == 1 or epoch % 500 == 0:\n",
        "      print('Epoch: ', epoch, 'Training Loss: ', loss_train, 'Test Loss: ', loss_test)\n",
        "  return params\n",
        "\n",
        "# First SGD optimizer at lr = 0.0001\n",
        "print('First SGD optimizer at learning rate 0.0001')\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_1 = 0.0001\n",
        "optimizer_sgd_1 = optim.SGD([params], lr = learning_rate_1)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_sgd_1,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# First Adam optimizer at lr = 0.0001\n",
        "print('First Adam optimizer at learning rate 0.0001')\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_1 = 0.0001\n",
        "optimizer_Adam_1 = optim.Adam([params], lr = learning_rate_1)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_Adam_1,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# Second SGD optimizer at lr = 0.00001\n",
        "print('Second SGD optimizer at learning rate 0.00001')\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_2 = 0.00001\n",
        "optimizer_sgd_2 = optim.SGD([params], lr = learning_rate_2)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_sgd_2,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# Second Adam optimizer at lr = 0.00001\n",
        "print('Second Adam optimizer at learning rate 0.00001')\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_2 = 0.00001\n",
        "optimizer_Adam_2 = optim.Adam([params], lr = learning_rate_2)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_Adam_2,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# Third SGD optimizer at lr = 0.000001\n",
        "print('Third SGD optimizer at learning rate 0.000001')\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_3 = 0.000001\n",
        "optimizer_sgd_3 = optim.SGD([params], lr = learning_rate_3)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_sgd_3,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# Third Adam optimizer at lr = 0.000001\n",
        "print('Third Adam optimizer at learning rate 0.000001')\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_3 = 0.000001\n",
        "optimizer_Adam_3 = optim.Adam([params], lr = learning_rate_3)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_Adam_3,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# Fourth SGD optimizer at lr = 0.0000001\n",
        "print('Fourth SGD optimizer at learning rate 0.0000001')\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_4 = 0.0000001\n",
        "optimizer_sgd_4= optim.SGD([params], lr = learning_rate_4)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_sgd_4,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# Fourth Adam optimizer at lr = 0.0000001\n",
        "print('Fourth Adam optimizer at learning rate 0.0000001')\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_4 = 0.0000001\n",
        "optimizer_Adam_4 = optim.Adam([params], lr = learning_rate_4)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_Adam_4,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "AXY8gCWVbNoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44828a71-cc97-42ae-b18e-01778c0b35a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "First SGD optimizer at learning rate 0.0001\n",
            "Epoch:  1 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  500 Training Loss:  tensor(2.1270e+18, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(7.6169e+18, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(1.0923e+25, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(3.9152e+25, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(5.6094e+31, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.0106e+32, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(2.8807e+38, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(1.0325e+39, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(1.4793e+45, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(5.3023e+45, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(7.5970e+51, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.7230e+52, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(3.9013e+58, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(1.3983e+59, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(2.0035e+65, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(7.1811e+65, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(1.0289e+72, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(3.6878e+72, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(nan, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(nan, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "First Adam optimizer at learning rate 0.0001\n",
            "Epoch:  1 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5189e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5189e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5189e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5189e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5189e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Second SGD optimizer at learning rate 0.00001\n",
            "Epoch:  1 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  500 Training Loss:  tensor(2.4073e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.2654e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(2.2760e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.1517e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(2.1833e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.0978e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(2.1132e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.0706e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(2.0566e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.0528e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(2.0080e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.0364e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(1.9645e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.0177e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(1.9243e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(1.9955e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(1.8862e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(1.9699e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(1.8499e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(1.9413e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Second Adam optimizer at learning rate 0.00001\n",
            "Epoch:  1 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Third SGD optimizer at learning rate 0.000001\n",
            "Epoch:  1 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  500 Training Loss:  tensor(2.5805e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4694e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(2.5577e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4397e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(2.5358e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4120e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(2.5150e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.3863e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(2.4950e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.3623e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(2.4759e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.3399e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(2.4576e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.3192e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(2.4400e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.2998e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(2.4232e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.2818e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(2.4070e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.2651e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Third Adam optimizer at learning rate 0.000001\n",
            "Epoch:  1 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Fourth SGD optimizer at learning rate 0.0000001\n",
            "Epoch:  1 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  500 Training Loss:  tensor(2.6076e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4694e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(2.6002e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4797e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(2.5971e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4854e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(2.5946e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4859e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(2.5922e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4842e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(2.5899e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4815e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(2.5875e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4786e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(2.5851e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4755e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(2.5828e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4724e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(2.5804e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.4693e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Fourth Adam optimizer at learning rate 0.0000001\n",
            "Epoch:  1 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(2.6470e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Test Loss:  tensor(2.5190e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 9.9940e-01,  9.9940e-01,  9.9940e-01,  9.9940e-01,  9.9940e-01,\n",
              "        -5.0000e-04], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 3\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch.optim as optim\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/Housing.csv'\n",
        "housing = (pd.read_csv(file_path))\n",
        "housing.head()\n",
        "\n",
        "#cols = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "#variable_list = ['parking']\n",
        "# Create the list of variables within housing\n",
        "variable_list = ['area','bedrooms','bathrooms','stories','parking','mainroad','guestroom','basement','hotwaterheating','airconditioning','prefarea','furnishingstatus']\n",
        "#Turn Yes and No into 1 and 0 only on variables in variable_change_list\n",
        "variable_change_list = ['basement','airconditioning','prefarea','furnishingstatus','mainroad','guestroom','hotwaterheating']\n",
        "def data_map(x):\n",
        "    return x.map({'yes': 1, 'no': 0, 'unfurnished': -1, 'semi-furnished': 0, 'furnished': 1})\n",
        "# Apply the numerical conversions made when mapping\n",
        "housing[variable_change_list] = housing[variable_change_list].apply(data_map)\n",
        "housing.head()\n",
        "\n",
        "# Standardize the data\n",
        "standard = StandardScaler()\n",
        "housing_std = housing\n",
        "housing_std[variable_list] = standard.fit_transform(housing_std[variable_list])\n",
        "\n",
        "x = housing_std.drop(['price'], axis = 1)\n",
        "y = housing_std['price']\n",
        "#x.head()\n",
        "\n",
        "# Seperate housing data into training and test datasets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "x_train_numpy = np.c_[np.ones((len(y_train), 1)), x_train[variable_list]]\n",
        "x_test_numpy = np.c_[np.ones((len(y_test), 1)), x_test[variable_list]]\n",
        "\n",
        "x_train_tensor = torch.tensor(x_train_numpy)\n",
        "x_test_tensor = torch.tensor(x_test_numpy)\n",
        "y_train_tensor = torch.tensor(y_train.values)\n",
        "y_test_tensor = torch.tensor(y_test.values)\n",
        "\n",
        "y_train_mean = y_train_tensor.float().mean()\n",
        "y_train_standard = y_train.std()\n",
        "y_train_standard_tensor = (y_train_tensor - y_train_mean) / y_train_standard\n",
        "y_test_mean = y_test_tensor.float().mean()\n",
        "y_test_standard = y_test.std()\n",
        "y_test_standard_tensor = (y_test_tensor - y_test_mean) / y_test_standard\n",
        "\n",
        "def model(t_u, w12, w11, w10, w9, w8, w7, w6, w5, w4, w3, w2, w, b):\n",
        "  return w12*t_u**12 + w11*t_u**11 + w10*t_u**10 + w9*t_u**9 + w8*t_u*8 + w7*t_u**7 + w6*t_u**6 + w5*t_u**5 + w4*t_u**4 + w3*t_u**3 + w2*t_u**2 + w*t_u**1 + b\n",
        "\n",
        "def loss_fn(t_p, t_c):\n",
        "  mean = (t_p - t_c)**2\n",
        "  return mean.mean()\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "\n",
        "def training_loop(n_epochs, optimizer, params, t_u_train, t_c_train, t_u_test, t_c_test):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    if params.grad is not None:\n",
        "      params.grad.zero_()\n",
        "\n",
        "    t_p_train = model(t_u_train, *params)\n",
        "    loss_train = loss_fn(t_p_train.transpose(0, 1), t_c_train)\n",
        "    t_p_test = model(t_u_test, *params)\n",
        "    loss_test = loss_fn(t_p_test.transpose(0, 1), t_c_test)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print('Epoch: ', epoch, 'Training Loss: ', loss_train, 'Testing Loss: ', loss_test)\n",
        "\n",
        "  return params\n",
        "\n",
        "# First SGD at lr = 0.00000000000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_SGD_1 = 0.00000000000001\n",
        "optimizer_SGD_1 = optim.SGD([params], lr = learning_rate_SGD_1)\n",
        "print('First SGD at learning_rate = 0.00000000000001')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_SGD_1,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# First Adam at lr = 0.00000000000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_Adam_1 = 0.00000000000001\n",
        "optimizer_Adam_1 = optim.Adam([params], lr = learning_rate_Adam_1)\n",
        "print('First Adam at learning_rate = 0.00000000000001')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_Adam_1,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# Second SGD at lr = 0.000000000000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_SGD_2 = 0.000000000000001\n",
        "optimizer_SGD_2 = optim.SGD([params], lr = learning_rate_SGD_2)\n",
        "print('Second SGD at learning_rate = 0.000000000000001')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_SGD_2,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# Second Adam at lr = 0.000000000000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_Adam_2 = 0.000000000000001\n",
        "optimizer_Adam_2 = optim.Adam([params], lr = learning_rate_Adam_2)\n",
        "print('Second Adam at learning_rate = 0.000000000000001')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_Adam_2,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# Third SGD at lr = 0.0000000000000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_SGD_3 = 0.0000000000000001\n",
        "optimizer_SGD_3 = optim.SGD([params], lr = learning_rate_SGD_3)\n",
        "print('Third SGD at learning_rate = 0.0000000000000001')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_SGD_3,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# Third Adam at lr = 0.0000000000000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_Adam_3 = 0.0000000000000001\n",
        "optimizer_Adam_3 = optim.Adam([params], lr = learning_rate_Adam_3)\n",
        "print('Third Adam at learning_rate = 0.0000000000000001')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_Adam_3,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# Fourth SGD at lr = 0.00000000000000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_SGD_4 = 0.00000000000000001\n",
        "optimizer_SGD_4 = optim.SGD([params], lr = learning_rate_SGD_4)\n",
        "print('Fourth SGD at learning_rate = 0.00000000000000001')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_SGD_4,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "# Fourth Adam at lr = 0.00000000000000001\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "learning_rate_Adam_4 = 0.00000000000000001\n",
        "optimizer_Adam_4 = optim.Adam([params], lr = learning_rate_Adam_4)\n",
        "print('Fourth Adam at learning_rate = 0.00000000000000001')\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_Adam_4,\n",
        "    params = params,\n",
        "    t_u_train = x_train_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_u_test = x_test_tensor,\n",
        "    t_c_test = y_test_tensor,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtIOM2iB_9lc",
        "outputId": "97793d6a-4831-4023-97ad-fc44584257d1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "First SGD at learning_rate = 0.00000000000001\n",
            "Epoch:  500 Training Loss:  tensor(2.6316e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4938e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(2.6315e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4936e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(2.6314e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4935e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(2.6314e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4935e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(2.6313e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4937e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(2.6312e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4939e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(2.6312e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4943e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(2.6311e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4947e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(2.6311e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4951e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(2.6310e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4956e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "First Adam at learning_rate = 0.00000000000001\n",
            "Epoch:  500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Second SGD at learning_rate = 0.000000000000001\n",
            "Epoch:  500 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4942e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4941e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4941e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4940e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4940e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4940e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(2.6316e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4939e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(2.6316e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4939e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(2.6316e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4938e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(2.6316e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4938e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Second Adam at learning_rate = 0.000000000000001\n",
            "Epoch:  500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Third SGD at learning_rate = 0.0000000000000001\n",
            "Epoch:  500 Training Loss:  tensor(2.6329e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.5085e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4943e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4942e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4942e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4942e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4942e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4942e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4942e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4942e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(2.6317e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.4942e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Third Adam at learning_rate = 0.0000000000000001\n",
            "Epoch:  500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Fourth SGD at learning_rate = 0.00000000000000001\n",
            "Epoch:  500 Training Loss:  tensor(5.1686e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.1897e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(3.7108e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(1.0809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(3.0907e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(6.0713e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(2.8270e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.0420e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(2.7148e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(3.1696e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(2.6670e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.7927e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(2.6467e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.6284e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(2.6381e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.5560e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(2.6344e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.5236e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(2.6329e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(2.5087e+13, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Fourth Adam at learning_rate = 0.00000000000000001\n",
            "Epoch:  500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  1500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  2500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  3500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  4500 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
            "Epoch:  5000 Training Loss:  tensor(8.5855e+13, dtype=torch.float64, grad_fn=<MeanBackward0>) Testing Loss:  tensor(4.7809e+14, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
              "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
              "        4.9997e-14], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}